"""
Chat API - Step 6: LangChain + Natural Language Processing
ìì—°ì–´ â†’ QueryPlan â†’ Core API â†’ RenderSpec

Text-to-SQL ëª¨ë“œ ì¶”ê°€:
SQL_ENABLE_TEXT_TO_SQL=true ì„¤ì • ì‹œ AIê°€ ì§ì ‘ SQLì„ ìƒì„±í•˜ì—¬ ì½ê¸° ì „ìš© DBì—ì„œ ì‹¤í–‰
"""

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List, Generator, Tuple
from enum import Enum
import httpx
import logging
import os
import re
import uuid
import csv
import io
import math
from datetime import datetime

from app.constants.render_keywords import (
    CHART_KEYWORDS,
    TABLE_KEYWORDS,
    TIME_FIELD_KEYWORDS,
)


# ============================================
# ì°¸ì¡° í‘œí˜„ ê°ì§€ (ì—°ì† ëŒ€í™” WHERE ì¡°ê±´ ë³‘í•©ìš©)
# ============================================

class ReferenceType(str, Enum):
    """ì°¸ì¡° í‘œí˜„ ìœ í˜•"""
    LATEST = "latest"      # ì§ì „ ê²°ê³¼ ì°¸ì¡° ("ì´ì¤‘ì—", "ë°©ê¸ˆ")
    SPECIFIC = "specific"  # íŠ¹ì • ê²°ê³¼ ì°¸ì¡° ("30ê±´ì—ì„œ", "ì²« ë²ˆì§¸ ê²°ê³¼")
    PARTIAL = "partial"    # ë¶€ë¶„ ê²°ê³¼ ì°¸ì¡° ("ìƒìœ„ 10ê°œ", "ì²˜ìŒ 5ê±´")
    NONE = "none"          # ì°¸ì¡° í‘œí˜„ ì—†ìŒ


# í™•ì¥ëœ ì°¸ì¡° í‘œí˜„ íŒ¨í„´ (30ê°œ+)
# Phase 3: ì•”ì‹œì  í•„í„° íŒ¨í„´ ì¶”ê°€ (4ë‹¨ê³„+ ì²´ì´ë‹ ì§€ì›)
REFERENCE_PATTERNS = {
    ReferenceType.LATEST: [
        # í•œê¸€ - í‘œì¤€ í‘œí˜„
        r'ì´\s*ì¤‘ì—?ì„œ?',        # "ì´ì¤‘ì—", "ì´ ì¤‘ì—ì„œ", "ì´ì¤‘ì—ì„œ"
        r'ì—¬ê¸°ì„œ',               # "ì—¬ê¸°ì„œ"
        r'ê·¸\s*ì¤‘ì—?ì„œ?',        # "ê·¸ì¤‘ì—", "ê·¸ ì¤‘ì—ì„œ"
        r'ì§ì „\s*(ê²°ê³¼|ë°ì´í„°)?', # "ì§ì „", "ì§ì „ ê²°ê³¼"
        r'ë°©ê¸ˆ\s*(ê²°ê³¼|ë°ì´í„°)?', # "ë°©ê¸ˆ", "ë°©ê¸ˆ ê²°ê³¼"
        r'ìœ„\s*ê²°ê³¼',            # "ìœ„ ê²°ê³¼", "ìœ„ê²°ê³¼"
        r'ì•\s*(ì„œ|ì—ì„œ)',        # "ì•ì„œ", "ì•ì—ì„œ"
        r'í•´ë‹¹\s*ë°ì´í„°',         # "í•´ë‹¹ ë°ì´í„°"
        r'ì´\s*ê²°ê³¼ì—?ì„œ?',       # "ì´ ê²°ê³¼", "ì´ ê²°ê³¼ì—ì„œ"
        r'ì €\s*ì¤‘ì—?ì„œ?',         # "ì €ì¤‘ì—", "ì € ì¤‘ì—ì„œ"
        r'ê±°ê¸°ì„œ',               # "ê±°ê¸°ì„œ"
        # í•œê¸€ - êµ¬ì–´ì²´/ì¤„ì„ë§
        r'ì•„ê¹Œ\s*(ê·¸\s*)?(ê±°|ê²ƒ|ë°ì´í„°)?', # "ì•„ê¹Œ ê·¸ê±°", "ì•„ê¹Œ ê±°"
        r'ê·¸ê±°ì—?ì„œ?',           # "ê·¸ê±°ì—ì„œ", "ê·¸ê±°ì„œ"
        r'ì´ê±°ì—?ì„œ?',           # "ì´ê±°ì—ì„œ", "ì´ê±°ì„œ"
        r'ì¡°íšŒ\s*í•œ\s*ê±°ì—?ì„œ?',  # "ì¡°íšŒí•œ ê±°ì—ì„œ"
        r'ë‚˜ì˜¨\s*(ê±°|ê²ƒ)ì—?ì„œ?',  # "ë‚˜ì˜¨ ê±°ì—ì„œ"
        r'ë³´ì—¬ì¤€\s*(ê±°|ê²ƒ)ì—?ì„œ?', # "ë³´ì—¬ì¤€ ê±°ì—ì„œ"
        r'ë°›ì€\s*(ê±°|ê²ƒ|ë°ì´í„°)ì—?ì„œ?', # "ë°›ì€ ê±°ì—ì„œ"
        r'í™”ë©´\s*(ì—\s*)?(ìˆëŠ”|ë³´ì´ëŠ”)', # "í™”ë©´ì— ìˆëŠ”"
        r'ì§€ê¸ˆ\s*(ë³´ì´ëŠ”|ìˆëŠ”)',  # "ì§€ê¸ˆ ë³´ì´ëŠ”"
        # í•œê¸€ - ìƒí™©ë³„ í‘œí˜„
        r'ì¡°íšŒëœ\s*ê²°ê³¼',         # "ì¡°íšŒëœ ê²°ê³¼"
        r'ì´ì „\s*ê²°ê³¼ì—?ì„œ?',     # "ì´ì „ ê²°ê³¼ì—ì„œ"
        r'ê²€ìƒ‰\s*ê²°ê³¼ì—?ì„œ?',     # "ê²€ìƒ‰ ê²°ê³¼ì—ì„œ"
        r'ëª©ë¡ì—?ì„œ?',           # "ëª©ë¡ì—ì„œ"
        r'í…Œì´ë¸”ì—?ì„œ?',         # "í…Œì´ë¸”ì—ì„œ"
        # ì˜ì–´/ì˜í•œ í˜¼ìš©
        r'from\s*(this|these|here)', # "from this", "from these"
        r'among\s*(this|these)',     # "among these"
        r'in\s*this\s*(result|data|list)', # "in this result"
        r'out\s*of\s*(this|these)',  # "out of these"
        r'(this|these)\s*ì¤‘ì—?ì„œ?',  # "these ì¤‘ì—ì„œ"

        # Phase 3: ì•”ì‹œì  í•„í„° íŒ¨í„´ (ë¬¸ì¥ ë "~ë§Œ" í‘œí˜„)
        # 4ë‹¨ê³„+ ì²´ì´ë‹ì—ì„œ "ê¸ˆì•¡ 10ë§Œì› ì´ìƒë§Œ" ê°™ì€ í‘œí˜„ ê°ì§€
        r'.{2,}(ê²ƒ|ê±´|ê±°|ë°ì´í„°)ë§Œ\s*$',  # "~ê²ƒë§Œ", "~ê±´ë§Œ", "~ê±°ë§Œ" ìœ¼ë¡œ ëë‚¨
        r'.{2,}(ì¸|í•œ|ëœ|ëŠ”)\s*ê²ƒë§Œ\s*$', # "~ì¸ ê²ƒë§Œ", "~í•œ ê²ƒë§Œ" ìœ¼ë¡œ ëë‚¨
        r'(ê¸ˆì•¡|amount).{0,15}(ì´ìƒ|ì´í•˜|ì´ˆê³¼|ë¯¸ë§Œ).{0,5}ë§Œ', # "ê¸ˆì•¡ X ì´ìƒë§Œ"
        r'(ìƒíƒœ|status).{0,10}(ì¸|ê°€|ë§Œ)',  # "ìƒíƒœê°€ Xì¸ ê²ƒë§Œ"
        r'(ê²°ì œ|method).{0,10}(ì¸|ê°€|ë§Œ)',  # "ê²°ì œìˆ˜ë‹¨ì´ Xì¸ ê²ƒë§Œ"
        r'(ê°€ë§¹ì |merchant).{0,10}(ë§Œ|ê²ƒë§Œ)', # "ê°€ë§¹ì  Xë§Œ"

        # Phase 3: ì•”ì‹œì  í•„í„° - ë¹„êµ/ë²”ìœ„ í‘œí˜„
        r'(ë§Œì›|ì›)\s*(ì´ìƒ|ì´í•˜|ì´ˆê³¼|ë¯¸ë§Œ)',  # "10ë§Œì› ì´ìƒ"
        r'\d+\s*(ì´ìƒ|ì´í•˜|ì´ˆê³¼|ë¯¸ë§Œ)\s*ë§Œ?$', # "100 ì´ìƒë§Œ"
        r'(í¬|ì‘|ë†’|ë‚®|ë§|ì )(ì€|ê³ )\s*(ê²ƒ|ê±´|ê±°)ë§Œ',  # "í° ê²ƒë§Œ", "ì‘ì€ ê±´ë§Œ"

        # Phase 3: í•„í„° ì¶”ê°€ í‘œí˜„
        r'ì¶”ê°€ë¡œ\s*.{0,10}(í•„í„°|ì¡°ê±´)',  # "ì¶”ê°€ë¡œ í•„í„°"
        r'(ì¡°ê±´|í•„í„°)\s*ì¶”ê°€',            # "ì¡°ê±´ ì¶”ê°€"
        r'ë”\s*ì¢í˜€',                    # "ë” ì¢í˜€ì„œ"
        r'ë²”ìœ„\s*ì¢í˜€',                  # "ë²”ìœ„ ì¢í˜€ì„œ"
    ],
    ReferenceType.SPECIFIC: [
        # íŠ¹ì • ê²°ê³¼ ì§€ì • (ìˆ«ì ì•ì— ë§¥ë½ í•„ìš”)
        r'ì•„ê¹Œ\s*\d+ê±´',         # "ì•„ê¹Œ 30ê±´"
        r'(ê·¸|ì €)\s*\d+ê±´ì—?ì„œ?', # "ê·¸ 30ê±´ì—ì„œ"
        r'(ì²«|ë‘|ì„¸)\s*ë²ˆì§¸\s*(ê²°ê³¼|ë°ì´í„°)', # "ì²« ë²ˆì§¸ ê²°ê³¼"
        r'(ì²˜ìŒ|ë§ˆì§€ë§‰)\s*(ê²°ê³¼|ë°ì´í„°)', # "ì²˜ìŒ ê²°ê³¼", "ë§ˆì§€ë§‰ ê²°ê³¼"
        r'(ì´ì „|ì•ì˜?)\s*ì¡°íšŒ',   # "ì´ì „ ì¡°íšŒ"
        r'ê²°ê³¼\s*\d+ê±´ì—?ì„œ?',   # "ê²°ê³¼ 30ê±´ì—ì„œ"
    ],
    ReferenceType.PARTIAL: [
        # ë¶€ë¶„ ê²°ê³¼ ì°¸ì¡°
        r'ìƒìœ„\s*\d+',           # "ìƒìœ„ 10ê°œ"
        r'í•˜ìœ„\s*\d+',           # "í•˜ìœ„ 5ê°œ"
        r'ì²˜ìŒ\s*\d+ê±´?',        # "ì²˜ìŒ 5ê±´"
        r'ìœ„\s*\d+ê±´?',          # "ìœ„ 10ê±´"
        r'top\s*\d+',            # "top 10"
        r'first\s*\d+',          # "first 5"
    ]
}

# ìƒˆ ì¿¼ë¦¬ íŒ¨í„´ (ì´ì „ ì¡°ê±´ ë¬´ì‹œ)
NEW_QUERY_PATTERNS = [
    r'ìƒˆë¡œ\s*.{0,10}ì¡°íšŒ',   # "ìƒˆë¡œ ì¡°íšŒ", "ìƒˆë¡œ í™˜ë¶ˆ ë‚´ì—­ ì¡°íšŒ"
    r'ë‹¤ì‹œ\s*.{0,10}ì¡°íšŒ',   # "ë‹¤ì‹œ ì¡°íšŒ", "ë‹¤ì‹œ ê²°ì œ ì¡°íšŒ"
    r'ì²˜ìŒë¶€í„°',             # "ì²˜ìŒë¶€í„°"
    r'ìƒˆ\s*ì¿¼ë¦¬',            # "ìƒˆ ì¿¼ë¦¬"
    r'ì „ì²´\s*ë‹¤ì‹œ',          # "ì „ì²´ ë‹¤ì‹œ"
    r'ì „ì²´\s*ì¡°íšŒ',          # "ì „ì²´ ì¡°íšŒ"
    r'ìƒˆë¡œ\s*ê²€ìƒ‰',          # "ìƒˆë¡œ ê²€ìƒ‰"
    r'ë‹¤ë¥¸\s*(ë°ì´í„°|ê²ƒ|ê±°)', # "ë‹¤ë¥¸ ë°ì´í„°"
    r'ë³„ë„ë¡œ',               # "ë³„ë„ë¡œ"
    r'fresh\s*query',        # "fresh query"
    r'new\s*search',         # "new search"
]

# ì§‘ê³„ í‚¤ì›Œë“œ íŒ¨í„´ (ì´ì „ ê²°ê³¼ ì°¸ì¡°ë¡œ ì²˜ë¦¬)
# ì´ì „ ëŒ€í™”ì—ì„œ ì¡°íšŒí•œ ê²°ê³¼ì— ëŒ€í•´ ì§‘ê³„í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
AGGREGATION_KEYWORDS = [
    # í•œê¸€ ì§‘ê³„ í‘œí˜„
    r'í•©ì‚°',                 # "í•©ì‚°í•´ì¤˜"
    r'í•©ê³„',                 # "í•©ê³„ ë³´ì—¬ì¤˜"
    r'ì´\s*(ê¸ˆì•¡|ê²°ì œ|ë§¤ì¶œ|ê±´ìˆ˜|ìˆ˜ëŸ‰)', # "ì´ ê¸ˆì•¡", "ì´ ê²°ì œ", "ì´ ë§¤ì¶œ"
    r'ì „ì²´\s*(ê¸ˆì•¡|ê²°ì œ|ë§¤ì¶œ)',  # "ì „ì²´ ê¸ˆì•¡" (ë‹¨, "ì „ì²´ ì¡°íšŒ"ëŠ” ì œì™¸)
    r'ë”í•´',                 # "ë”í•´ì¤˜"
    r'sum',                  # "sum êµ¬í•´ì¤˜"
    r'í‰ê· ',                 # "í‰ê·  êµ¬í•´ì¤˜"
    r'í‰ê· \s*(ê¸ˆì•¡|ê²°ì œ|ë§¤ì¶œ)',  # "í‰ê·  ê¸ˆì•¡"
    r'avg',                  # "avg êµ¬í•´ì¤˜"
    r'ê°œìˆ˜',                 # "ê°œìˆ˜ ì„¸ì¤˜"
    r'ëª‡\s*ê±´',              # "ëª‡ ê±´ì´ì•¼"
    r'ì¹´ìš´íŠ¸',               # "ì¹´ìš´íŠ¸ í•´ì¤˜"
    r'count',                # "count í•´ì¤˜"
    r'ìµœëŒ€\s*(ê¸ˆì•¡|ê°’)',      # "ìµœëŒ€ ê¸ˆì•¡"
    r'ìµœì†Œ\s*(ê¸ˆì•¡|ê°’)',      # "ìµœì†Œ ê¸ˆì•¡"
    r'max',                  # "max êµ¬í•´ì¤˜"
    r'min',                  # "min êµ¬í•´ì¤˜"
]


def detect_reference_expression(message: str) -> Tuple[bool, str]:
    """
    ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì°¸ì¡° í‘œí˜„ ê°ì§€

    ì°¸ì¡° í‘œí˜„ì´ ìˆìœ¼ë©´ ì´ì „ WHERE ì¡°ê±´ì„ ìœ ì§€í•´ì•¼ í•¨ì„ ì˜ë¯¸

    Args:
        message: ì‚¬ìš©ì ë©”ì‹œì§€

    Returns:
        (is_refinement, ref_type) íŠœí”Œ
        - is_refinement: Trueë©´ ì´ì „ ì¡°ê±´ ìœ ì§€ í•„ìš”
        - ref_type: 'filter' (í•„í„° ì¶”ê°€), 'aggregation' (ì§‘ê³„ ìš”ì²­), 'new' (ìƒˆ ì¿¼ë¦¬), 'none' (í•´ë‹¹ì—†ìŒ)
    """
    # ìƒˆ ì¿¼ë¦¬ íŒ¨í„´ ë¨¼ì € ì²´í¬ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    for pattern in NEW_QUERY_PATTERNS:
        if re.search(pattern, message, re.IGNORECASE):
            return (False, 'new')

    # ì°¸ì¡° íŒ¨í„´ ì²´í¬ (ìœ í˜•ë³„)
    for ref_type, patterns in REFERENCE_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return (True, 'filter')

    # ì§‘ê³„ í‚¤ì›Œë“œ ì²´í¬ (ì´ì „ ê²°ê³¼ì— ëŒ€í•œ ì§‘ê³„ë¡œ ì²˜ë¦¬)
    # ì§‘ê³„ ìš”ì²­ì€ ì´ì „ ëŒ€í™”ì—ì„œ ì¡°íšŒí•œ ê²°ê³¼ì— ëŒ€í•´ ìˆ˜í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
    for pattern in AGGREGATION_KEYWORDS:
        if re.search(pattern, message, re.IGNORECASE):
            return (True, 'aggregation')

    return (False, 'none')


def detect_reference_type(message: str) -> ReferenceType:
    """
    ì°¸ì¡° í‘œí˜„ì˜ ì„¸ë¶€ ìœ í˜• ê°ì§€

    Args:
        message: ì‚¬ìš©ì ë©”ì‹œì§€

    Returns:
        ReferenceType: ì°¸ì¡° ìœ í˜• (LATEST, SPECIFIC, PARTIAL, NONE)
    """
    # ìš°ì„ ìˆœìœ„: SPECIFIC > PARTIAL > LATEST
    for ref_type in [ReferenceType.SPECIFIC, ReferenceType.PARTIAL, ReferenceType.LATEST]:
        if ref_type in REFERENCE_PATTERNS:
            for pattern in REFERENCE_PATTERNS[ref_type]:
                if re.search(pattern, message, re.IGNORECASE):
                    return ref_type

    return ReferenceType.NONE

# Text-to-SQL ëª¨ë“œ í”Œë˜ê·¸
ENABLE_TEXT_TO_SQL = os.getenv("SQL_ENABLE_TEXT_TO_SQL", "false").lower() == "true"


def to_camel(string: str) -> str:
    """snake_caseë¥¼ camelCaseë¡œ ë³€í™˜"""
    components = string.split('_')
    return components[0] + ''.join(x.title() for x in components[1:])


def summarize_query_plan(query_plan: Dict[str, Any]) -> str:
    """QueryPlanì„ ê°„ë‹¨í•œ ìš”ì•½ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    parts = []

    entity = query_plan.get("entity", "")
    if entity:
        parts.append(entity)

    # í•„í„° ìš”ì•½
    filters = query_plan.get("filters", [])
    if filters:
        filter_strs = [f"{f.get('field')} {f.get('operator')} {f.get('value')}" for f in filters]
        parts.append(f"filters:[{', '.join(filter_strs)}]")

    # limit
    if query_plan.get("limit"):
        parts.append(f"limit:{query_plan['limit']}")

    # orderBy
    order_by = query_plan.get("orderBy", [])
    if order_by:
        order_strs = [f"{o.get('field')} {o.get('direction', 'asc')}" for o in order_by]
        parts.append(f"orderBy:[{', '.join(order_strs)}]")

    return ", ".join(parts) if parts else "[ì¿¼ë¦¬ ì—†ìŒ]"


def build_conversation_context(history: List["ChatMessageItem"]) -> str:
    """ì´ì „ ëŒ€í™”ë¥¼ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (êµ¬ì¡°í™”ëœ ê²°ê³¼ í˜„í™© í¬í•¨)"""
    if not history:
        return ""

    context = "## ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸\n\n"

    # ============================================
    # ì¡°íšŒ ê²°ê³¼ í˜„í™© (êµ¬ì¡°í™”ëœ í…Œì´ë¸” í˜•ì‹)
    # ============================================
    result_messages = []
    for i, msg in enumerate(history):
        if msg.role == "assistant" and msg.queryResult:
            entity = msg.queryPlan.get("entity", "unknown") if msg.queryPlan else "unknown"
            count = msg.queryResult.get("totalCount", 0)

            # í•„í„° ì •ë³´ ì¶”ì¶œ
            filters = msg.queryPlan.get("filters", []) if msg.queryPlan else []
            filter_desc = "-"
            if filters:
                filter_strs = [f"{f.get('field')}={f.get('value')}" for f in filters[:2]]
                filter_desc = ", ".join(filter_strs)

            # ê¸ˆì•¡ ì •ë³´ ì¶”ì¶œ
            total_amount = None
            data_obj = msg.queryResult.get("data", {})
            rows = data_obj.get("rows", []) if isinstance(data_obj, dict) else []
            if rows:
                amounts = []
                for row in rows:
                    if isinstance(row, dict):
                        for field in ["amount", "totalAmount", "total_amount", "price"]:
                            if field in row and row[field] is not None:
                                try:
                                    amounts.append(float(row[field]))
                                except (ValueError, TypeError):
                                    pass
                                break
                if amounts:
                    total_amount = sum(amounts)

            # ê²°ê³¼ íƒ€ì… íŒë‹¨ (í…Œì´ë¸” vs ì§‘ê³„)
            result_type = "table"  # ê¸°ë³¸ê°’
            if msg.renderSpec and msg.renderSpec.get("type") == "text":
                result_type = "aggregation"

            # ê´€ê³„ ì •ë³´ ì¶”ì •
            relation = "ìµœì´ˆ ì¡°íšŒ"
            if len(result_messages) > 0:
                prev = result_messages[-1]
                if prev["entity"] == entity and filters:
                    relation = f"#{prev['index']}ì—ì„œ í•„í„°ë§"
                elif prev["entity"] != entity:
                    relation = "ìƒˆë¡œìš´ ì—”í‹°í‹°"
                else:
                    relation = "ì¡°ê±´ ë³€ê²½"

            result_messages.append({
                "index": i,
                "entity": entity,
                "count": count,
                "filter_desc": filter_desc,
                "total_amount": total_amount,
                "result_type": result_type,
                "relation": relation,
                "is_latest": False
            })

    if result_messages:
        result_messages[-1]["is_latest"] = True  # ë§ˆì§€ë§‰ì´ ì§ì „ ê²°ê³¼

        # êµ¬ì¡°í™”ëœ í…Œì´ë¸” í˜•ì‹
        context += "### ğŸ“Š ì¡°íšŒ ê²°ê³¼ í˜„í™©\n"
        context += "| # | ì—”í‹°í‹° | ê±´ìˆ˜ | ì¡°ê±´ | ê¸ˆì•¡ | íƒ€ì… | ê´€ê³„ |\n"
        context += "|---|--------|------|------|------|------|------|\n"

        for r in result_messages:
            marker = "ğŸ‘‰" if r["is_latest"] else ""
            amount_str = f"${r['total_amount']:,.0f}" if r['total_amount'] else "-"
            context += f"| {marker}{r['index']} | {r['entity']} | {r['count']} | {r['filter_desc']} | {amount_str} | {r['result_type']} | {r['relation']} |\n"

        context += "\n"

        # ============================================
        # ê²°ê³¼ ê´€ê³„ ë¶„ì„ (LLMì´ ì´í•´í•˜ê¸° ì‰½ê²Œ)
        # ============================================
        context += "### ê²°ê³¼ ê´€ê³„ ë¶„ì„\n"
        entities = {}
        for r in result_messages:
            if r["entity"] not in entities:
                entities[r["entity"]] = []
            entities[r["entity"]].append(r)

        for entity, results in entities.items():
            if len(results) > 1:
                context += f"- **{entity}**: {len(results)}ê°œ ê²°ê³¼ (ì¡°ê±´ì´ ë‹¤ë¦„)\n"
                for r in results[1:]:
                    context += f"  - ê²°ê³¼ #{r['index']}ì€ #{result_messages[0]['index']}ì—ì„œ íŒŒìƒë¨\n"
            else:
                context += f"- **{entity}**: 1ê°œ ê²°ê³¼\n"

        context += "\n"

        # ============================================
        # ê³„ì‚°ì— ì‚¬ìš©í•  ë°ì´í„° (ëª…ì‹œì )
        # ============================================
        latest = result_messages[-1]
        context += "### ğŸ“Œ í˜„ì¬ ì‘ì—… ëŒ€ìƒ (ì§ì „ ê²°ê³¼)\n"
        context += f"- **ì—”í‹°í‹°**: {latest['entity']}\n"
        context += f"- **ê±´ìˆ˜**: {latest['count']}ê±´\n"
        context += f"- **íƒ€ì…**: {latest['result_type']} ({'ëª©ë¡ ë°ì´í„°' if latest['result_type'] == 'table' else 'ì§‘ê³„ ê²°ê³¼'})\n"
        if latest['total_amount']:
            context += f"- **ê¸ˆì•¡ í•©ê³„**: ${latest['total_amount']:,.0f}\n"
        if latest['filter_desc'] != "-":
            context += f"- **ì ìš©ëœ í•„í„°**: {latest['filter_desc']}\n"

        context += "\n"

        # ë‹¤ì¤‘ ê²°ê³¼ ê²½ê³ 
        if len(result_messages) > 1:
            entity_set = set(r["entity"] for r in result_messages)
            if len(entity_set) > 1:
                context += f"âš ï¸ **ì£¼ì˜**: ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê²°ê³¼ê°€ {len(result_messages)}ê°œ ìˆìŠµë‹ˆë‹¤ ({', '.join(entity_set)})\n"
                context += "â†’ ì°¸ì¡° í‘œí˜„ ì—†ìœ¼ë©´ ì–´ë–¤ ê²°ê³¼ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ”ì§€ ë¶ˆëª…í™•í•  ìˆ˜ ìˆìŒ\n\n"

    # ============================================
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœê·¼ 5ê°œ)
    # ============================================
    context += "### ëŒ€í™” íˆìŠ¤í† ë¦¬\n"
    for msg in history[-5:]:
        if msg.role == 'user':
            context += f"**ì‚¬ìš©ì**: {msg.content}\n"
        else:
            # queryPlan ìš”ì•½ í¬í•¨
            if msg.queryPlan:
                plan_summary = summarize_query_plan(msg.queryPlan)
                context += f"**ì–´ì‹œìŠ¤í„´íŠ¸**: [ì¿¼ë¦¬: {plan_summary}]\n"
            else:
                context += f"**ì–´ì‹œìŠ¤í„´íŠ¸**: [ê²°ê³¼ í‘œì‹œë¨]\n"

            # ì§‘ê³„ ê²°ê³¼ê°’ í¬í•¨ (ì¤‘ìš”: í›„ì† ê³„ì‚°ìš©)
            if msg.renderSpec and msg.renderSpec.get("type") == "text":
                text_content = msg.renderSpec.get("text", {}).get("content", "")
                if text_content and ("í•©ê³„" in text_content or "$" in text_content or "ì›" in text_content):
                    context += f"  â†’ **ì§‘ê³„ ê²°ê³¼**: {text_content}\n"

    # ============================================
    # í›„ì† ì§ˆë¬¸ ì²˜ë¦¬ ê·œì¹™ (ê°•í™”)
    # ============================================
    context += "\n### í›„ì† ì§ˆë¬¸ ì²˜ë¦¬ ê·œì¹™\n"
    context += "1. **ì°¸ì¡° í‘œí˜„ ìˆìŒ** ('ì´ì¤‘ì—', 'ì—¬ê¸°ì„œ', 'ì§ì „', 'ë°©ê¸ˆ', 'ì•„ê¹Œ ê·¸ê±°') â†’ ì§ì „ ê²°ê³¼ ì‚¬ìš©\n"
    context += "2. **ì°¸ì¡° í‘œí˜„ ì—†ìŒ** + ë‹¤ì¤‘ ê²°ê³¼ â†’ ë¬¸ë§¥ìƒ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ clarification ê³ ë ¤\n"
    context += "3. **ì§ì „ ê²°ê³¼ íƒ€ì… í™•ì¸**:\n"
    context += "   - í…Œì´ë¸”(ëª©ë¡) + 'í•©ì‚°' â†’ aggregate_local\n"
    context += "   - ì§‘ê³„ê²°ê³¼ + 'ìˆ˜ìˆ˜ë£Œ ì ìš©' â†’ direct_answer\n"
    context += "   - ì§‘ê³„ê²°ê³¼ + 'í•„í„°ë§' â†’ query_needed (ì§‘ê³„ ê²°ê³¼ëŠ” í•„í„° ë¶ˆê°€)\n"
    context += "4. **ì—”í‹°í‹° ìœ ì§€**: í›„ì† ì§ˆë¬¸ì—ì„œ ë‹¤ë¥¸ ì—”í‹°í‹°ë¡œ ë³€ê²½í•˜ë ¤ë©´ ëª…ì‹œì  í‘œí˜„ í•„ìš”\n"

    return context


def get_previous_query_plan(history: List["ChatMessageItem"]) -> Optional[Dict[str, Any]]:
    """ì´ì „ ëŒ€í™”ì—ì„œ ë§ˆì§€ë§‰ queryPlan ì¶”ì¶œ"""
    if not history:
        return None

    # ì—­ìˆœìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ê°€ì¥ ìµœê·¼ assistantì˜ queryPlan ì°¾ê¸°
    for msg in reversed(history):
        if msg.role == 'assistant' and msg.queryPlan:
            return msg.queryPlan
    return None


def extract_previous_results(history: List["ChatMessageItem"]) -> List[Dict[str, Any]]:
    """ì´ì „ ëŒ€í™”ì—ì„œ ì¡°íšŒ/ì§‘ê³„ ê²°ê³¼ ìš”ì•½ ì¶”ì¶œ (Intent Classificationìš©)

    ì‹¤ì œ ë°ì´í„° ê°’ë„ ì¶”ì¶œí•˜ì—¬ LLMì´ ê³„ì‚°í•  ìˆ˜ ìˆë„ë¡ í•¨
    """
    results = []
    if not history:
        return results

    for i, msg in enumerate(history):
        if msg.role == "assistant":
            result_info = {
                "index": i,
                "entity": None,
                "count": 0,
                "aggregation": None,
                "data_summary": None,  # ì‹¤ì œ ë°ì´í„° ìš”ì•½
                "total_amount": None   # ê¸ˆì•¡ í•©ê³„ (ìˆëŠ” ê²½ìš°)
            }

            # QueryResultê°€ ìˆìœ¼ë©´ ì¡°íšŒ ê²°ê³¼
            if msg.queryResult:
                logger.info(f"[extract_previous_results] msg #{i} has queryResult with keys: {list(msg.queryResult.keys())}")
                result_info["count"] = msg.queryResult.get("totalCount", 0)
                if msg.queryPlan:
                    result_info["entity"] = msg.queryPlan.get("entity", "unknown")

                # ì‹¤ì œ ë°ì´í„°ì—ì„œ ê¸ˆì•¡ í•©ê³„ ì¶”ì¶œ
                data_obj = msg.queryResult.get("data", {})
                # data is an object with 'rows' property according to query-result.schema.json
                rows = data_obj.get("rows", []) if isinstance(data_obj, dict) else []
                logger.info(f"[extract_previous_results] msg #{i} rows length: {len(rows) if rows else 0}")

                if rows:
                    # amount í•„ë“œê°€ ìˆìœ¼ë©´ í•©ê³„ ê³„ì‚°
                    amounts = []
                    for row_idx, row in enumerate(rows):
                        if isinstance(row, dict):
                            logger.info(f"[extract_previous_results] msg #{i} row #{row_idx} keys: {list(row.keys())}")
                            # amount, totalAmount, ê¸ˆì•¡ ë“± ë‹¤ì–‘í•œ í•„ë“œëª… ì²´í¬
                            for field in ["amount", "totalAmount", "total_amount", "price", "ê¸ˆì•¡"]:
                                if field in row and row[field] is not None:
                                    try:
                                        amounts.append(float(row[field]))
                                        logger.info(f"[extract_previous_results] msg #{i} row #{row_idx} found {field}={row[field]}")
                                    except (ValueError, TypeError) as e:
                                        logger.info(f"[extract_previous_results] msg #{i} row #{row_idx} error converting {field}: {e}")
                                        pass
                                    break
                        else:
                            logger.info(f"[extract_previous_results] msg #{i} row #{row_idx} is not a dict: {type(row)}")

                    if amounts:
                        result_info["total_amount"] = sum(amounts)
                        result_info["data_summary"] = f"ê¸ˆì•¡ í•©ê³„: ${result_info['total_amount']:,.0f} ({len(amounts)}ê±´)"
                        logger.info(f"[extract_previous_results] msg #{i} extracted total_amount: ${result_info['total_amount']:,.0f} from {len(amounts)} amounts")
                    else:
                        logger.info(f"[extract_previous_results] msg #{i} no amounts found in {len(rows)} rows")

            # RenderSpecì´ text íƒ€ì…ì´ë©´ ì§‘ê³„ ê²°ê³¼ì¼ ìˆ˜ ìˆìŒ
            if msg.renderSpec and msg.renderSpec.get("type") == "text":
                text_content = msg.renderSpec.get("text", {}).get("content", "")
                if text_content:
                    result_info["aggregation"] = text_content

                    # í…ìŠ¤íŠ¸ì—ì„œ ê¸ˆì•¡ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: ê´„í˜¸ ì•ˆ ì „ì²´ ê¸ˆì•¡ > ì¶•ì•½ ê¸ˆì•¡)
                    if result_info["total_amount"] is None:
                        # 1ìˆœìœ„: ê´„í˜¸ ì•ˆì˜ ì „ì²´ ê¸ˆì•¡ (ì˜ˆ: "$2.88M ($2,878,000)" â†’ 2878000)
                        full_amount_match = re.search(r'\(\$?([\d,]+)\)', text_content)
                        if full_amount_match:
                            try:
                                result_info["total_amount"] = float(full_amount_match.group(1).replace(',', ''))
                                logger.info(f"[extract_previous_results] Extracted full amount from parens: ${result_info['total_amount']:,.0f}")
                            except ValueError:
                                pass

                        # 2ìˆœìœ„: M/K ì ‘ë¯¸ì‚¬ ì²˜ë¦¬ (ì˜ˆ: "$2.88M" â†’ 2880000)
                        if result_info["total_amount"] is None:
                            abbrev_match = re.search(r'\$?([\d,]+(?:\.\d+)?)\s*([MK])', text_content)
                            if abbrev_match:
                                try:
                                    value = float(abbrev_match.group(1).replace(',', ''))
                                    suffix = abbrev_match.group(2)
                                    if suffix == 'M':
                                        value *= 1_000_000
                                    elif suffix == 'K':
                                        value *= 1_000
                                    result_info["total_amount"] = value
                                    logger.info(f"[extract_previous_results] Extracted abbreviated amount: ${result_info['total_amount']:,.0f}")
                                except ValueError:
                                    pass

                        # 3ìˆœìœ„: ì¼ë°˜ ê¸ˆì•¡ (ì˜ˆ: "$1,234,567")
                        if result_info["total_amount"] is None:
                            simple_match = re.search(r'\$?([\d,]+(?:\.\d+)?)', text_content)
                            if simple_match:
                                try:
                                    result_info["total_amount"] = float(simple_match.group(1).replace(',', ''))
                                    logger.info(f"[extract_previous_results] Extracted simple amount: ${result_info['total_amount']:,.0f}")
                                except ValueError:
                                    pass

            # ì¡°íšŒ ê²°ê³¼ë‚˜ ì§‘ê³„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if result_info["count"] > 0 or result_info["aggregation"]:
                results.append(result_info)
                logger.info(f"[extract_previous_results] Added result #{len(results)}: entity={result_info['entity']}, count={result_info['count']}, total_amount={result_info['total_amount']}")

    logger.info(f"[extract_previous_results] Total results extracted: {len(results)}")
    return results


def merge_filters(previous_plan: Dict[str, Any], new_plan: Dict[str, Any]) -> Dict[str, Any]:
    """ì´ì „ í•„í„°ì™€ ìƒˆ í•„í„°ë¥¼ ë³‘í•©"""
    if not previous_plan:
        return new_plan

    # clarification ìš”ì²­ì´ë©´ ë³‘í•©í•˜ì§€ ì•ŠìŒ
    if new_plan.get("needs_clarification"):
        return new_plan

    # ì´ì „ í•„í„° ê°€ì ¸ì˜¤ê¸°
    prev_filters = previous_plan.get("filters", [])
    new_filters = new_plan.get("filters", [])

    # ìƒˆ í•„í„°ì˜ í•„ë“œëª… ëª©ë¡
    new_filter_fields = {f.get("field") for f in new_filters}

    # ì´ì „ í•„í„° ì¤‘ ìƒˆ í•„í„°ì— ì—†ëŠ” ê²ƒë§Œ ë³‘í•© (ì¤‘ë³µ í•„ë“œ ë°©ì§€)
    merged_filters = list(new_filters)  # ìƒˆ í•„í„° ìš°ì„ 
    for prev_filter in prev_filters:
        if prev_filter.get("field") not in new_filter_fields:
            merged_filters.append(prev_filter)

    # ë³‘í•©ëœ ê²°ê³¼
    merged_plan = dict(new_plan)
    if merged_filters:
        merged_plan["filters"] = merged_filters

    # ì´ì „ entity ìœ ì§€ (ìƒˆ planì— entityê°€ ì—†ìœ¼ë©´)
    if not merged_plan.get("entity") and previous_plan.get("entity"):
        merged_plan["entity"] = previous_plan["entity"]

    # ì´ì „ limit ìœ ì§€ (ìƒˆ planì´ ê¸°ë³¸ê°’ 10ì´ë©´)
    if merged_plan.get("limit") == 10 and previous_plan.get("limit"):
        merged_plan["limit"] = previous_plan["limit"]

    return merged_plan

from app.services.query_planner import get_query_planner, IntentType
from app.services.render_composer import get_render_composer
from app.services.rag_service import get_rag_service

# Text-to-SQL ëª¨ë“œìš© import (ì¡°ê±´ë¶€)
if ENABLE_TEXT_TO_SQL:
    from app.services.text_to_sql import get_text_to_sql_service, extract_where_conditions

logger = logging.getLogger(__name__)
logger.info(f"Text-to-SQL mode: {'ENABLED' if ENABLE_TEXT_TO_SQL else 'DISABLED'}")

router = APIRouter()

# Configuration
CORE_API_URL = os.getenv("CORE_API_URL", "http://localhost:8080")
ENABLE_QUERY_PLAN_VALIDATION = os.getenv("ENABLE_QUERY_PLAN_VALIDATION", "true").lower() == "true"


class ChatMessageItem(BaseModel):
    """ëŒ€í™” ë©”ì‹œì§€ ì•„ì´í…œ"""
    id: str
    role: str  # 'user' | 'assistant'
    content: str
    timestamp: str
    status: Optional[str] = None
    renderSpec: Optional[Dict[str, Any]] = None
    queryResult: Optional[Dict[str, Any]] = None
    queryPlan: Optional[Dict[str, Any]] = None  # ì´ì „ ì¿¼ë¦¬ ì¡°ê±´ ì €ì¥ìš©


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­"""
    message: str
    conversation_id: Optional[str] = None
    session_id: Optional[str] = Field(default=None, alias="sessionId")
    conversation_history: Optional[List[ChatMessageItem]] = Field(default=None, alias="conversationHistory")

    class Config:
        populate_by_name = True


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ - UI íƒ€ì…ê³¼ ì¼ì¹˜"""
    request_id: str = Field(alias="requestId")
    render_spec: Dict[str, Any] = Field(alias="renderSpec")
    query_result: Optional[Dict[str, Any]] = Field(default=None, alias="queryResult")  # filter_local ì‹œ None ê°€ëŠ¥
    query_plan: Dict[str, Any] = Field(alias="queryPlan")  # ì´ë²ˆ ì¿¼ë¦¬ ì¡°ê±´ (í›„ì† ì§ˆë¬¸ìš©)
    ai_message: Optional[str] = Field(default=None, alias="aiMessage")
    timestamp: str

    class Config:
        populate_by_name = True


@router.post("/chat", response_model=ChatResponse, response_model_by_alias=True)
async def chat(request: ChatRequest):
    """
    Step 6: LangChain ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬

    Flow (ê¸°ì¡´ QueryPlan ëª¨ë“œ):
    1. ì‚¬ìš©ì ë©”ì‹œì§€ ìˆ˜ì‹ 
    2. QueryPlannerServiceë¡œ ìì—°ì–´ â†’ QueryPlan ë³€í™˜
    3. Core API í˜¸ì¶œ
    4. RenderComposerServiceë¡œ QueryResult â†’ RenderSpec ë³€í™˜
    5. RenderSpec ë°˜í™˜

    Flow (Text-to-SQL ëª¨ë“œ, SQL_ENABLE_TEXT_TO_SQL=true):
    1. ì‚¬ìš©ì ë©”ì‹œì§€ ìˆ˜ì‹ 
    2. TextToSqlServiceë¡œ ìì—°ì–´ â†’ SQL ë³€í™˜
    3. SQL ê²€ì¦ (SqlValidator)
    4. ì½ê¸° ì „ìš© DBì—ì„œ ì§ì ‘ ì‹¤í–‰
    5. ê²°ê³¼ë¥¼ RenderSpecìœ¼ë¡œ ë³€í™˜
    """
    start_time = datetime.utcnow()
    conversation_id = request.conversation_id or str(uuid.uuid4())
    request_id = f"req-{uuid.uuid4().hex[:8]}"

    logger.info(f"[{request_id}] Received message: {request.message}")

    processing_info = {
        "requestId": request_id,
        "stages": []
    }

    # ========================================
    # Text-to-SQL ëª¨ë“œ ë¶„ê¸°
    # ========================================
    if ENABLE_TEXT_TO_SQL:
        return await handle_text_to_sql(request, request_id, start_time)

    try:
        # Stage 0: Intent Classification (2ë‹¨ê³„ ë¶„ë¥˜)
        stage_start = datetime.utcnow()
        query_planner = get_query_planner()

        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        conversation_context = None
        previous_results = []
        if request.conversation_history:
            conversation_context = build_conversation_context(request.conversation_history)
            previous_results = extract_previous_results(request.conversation_history)
            logger.info(f"[{request_id}] Using conversation context with {len(request.conversation_history)} messages")
            logger.info(f"[{request_id}] Found {len(previous_results)} previous results for intent classification")

        # 1ë‹¨ê³„: Intent ë¶„ë¥˜ (ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë¹ ë¥´ê²Œ)
        intent_result = await query_planner.classify_intent(
            request.message,
            conversation_context or "",
            previous_results
        )
        logger.info(f"[{request_id}] Intent classification: {intent_result.intent.value}, confidence={intent_result.confidence:.2f}")

        # direct_answerë©´ ë°”ë¡œ ì‘ë‹µ ë°˜í™˜ (QueryPlan ìƒì„± ìŠ¤í‚µ)
        if intent_result.intent == IntentType.DIRECT_ANSWER and intent_result.direct_answer_text:
            logger.info(f"[{request_id}] Direct answer detected, skipping QueryPlan generation")
            logger.info(f"[{request_id}] Direct answer text: {intent_result.direct_answer_text}")

            return ChatResponse(
                request_id=request_id,
                query_plan={
                    "query_intent": "direct_answer",
                    "requestId": request_id
                },
                query_result=None,
                render_spec={
                    "type": "text",
                    "text": {
                        "content": intent_result.direct_answer_text,
                        "format": "markdown"
                    },
                    "metadata": {
                        "intent": "direct_answer",
                        "confidence": intent_result.confidence,
                        "reasoning": intent_result.reasoning
                    }
                },
                timestamp=datetime.utcnow().isoformat() + "Z"
            )

        processing_info["stages"].append({
            "name": "Intent Classification",
            "duration": (datetime.utcnow() - stage_start).total_seconds() * 1000,
            "result": intent_result.intent.value
        })

        # Stage 1: Natural Language â†’ QueryPlan
        stage_start = datetime.utcnow()

        query_plan = await query_planner.generate_query_plan(
            request.message,
            conversation_context=conversation_context,
            enable_validation=ENABLE_QUERY_PLAN_VALIDATION
        )

        # LLMì´ íŒë‹¨í•œ ì˜ë„ì— ë”°ë¼ í•„í„° ë³‘í•© ê²°ì •
        query_intent = query_plan.get("query_intent", "new_query")
        logger.info(f"[{request_id}] Query intent: {query_intent}")

        if query_intent == "refine_previous":
            if request.conversation_history:
                previous_plan = get_previous_query_plan(request.conversation_history)
                if previous_plan:
                    logger.info(f"[{request_id}] Intent: refine_previous, merging with previous filters")
                    logger.info(f"[{request_id}] Previous plan: {previous_plan}")
                    query_plan = merge_filters(previous_plan, query_plan)
                    logger.info(f"[{request_id}] Merged plan: {query_plan}")
        elif query_intent == "filter_local":
            # í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ í•„í„°ë§: Core API í˜¸ì¶œ ì—†ì´ í•„í„° ì¡°ê±´ë§Œ ë°˜í™˜
            logger.info(f"[{request_id}] Intent: filter_local, client-side filtering")

            # entityê°€ ì—†ìœ¼ë©´ ì´ì „ queryPlanì—ì„œ ìƒì†
            if not query_plan.get("entity") and request.conversation_history:
                previous_plan = get_previous_query_plan(request.conversation_history)
                if previous_plan and previous_plan.get("entity"):
                    query_plan["entity"] = previous_plan["entity"]
                    logger.info(f"[{request_id}] Inherited entity from previous plan: {query_plan['entity']}")

            # ì´ì „ ê²°ê³¼ê°€ ìˆëŠ” ë©”ì‹œì§€ë“¤ ì°¾ê¸°
            result_messages = []
            if request.conversation_history:
                logger.info(f"[{request_id}] Checking {len(request.conversation_history)} messages in history")
                for i, msg in enumerate(request.conversation_history):
                    has_query_result = msg.queryResult is not None
                    logger.info(f"[{request_id}] Message {i}: role={msg.role}, hasQueryResult={has_query_result}")
                    if msg.role == "assistant" and msg.queryResult:
                        result_messages.append((i, msg))

            logger.info(f"[{request_id}] Found {len(result_messages)} result messages")

            # 1ë‹¨ê³„: LLMì´ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨í–ˆëŠ”ì§€ í™•ì¸
            needs_result_clarification = query_plan.get("needs_result_clarification", False)
            logger.info(f"[{request_id}] 1st stage LLM decision: needs_result_clarification={needs_result_clarification}")

            # 2ë‹¨ê³„: ë‹¤ì¤‘ ê²°ê³¼ + 1ë‹¨ê³„ê°€ Falseë©´ ìƒìœ„ ëª¨ë¸ë¡œ ì¬íŒë‹¨
            if len(result_messages) > 1 and not needs_result_clarification:
                logger.info(f"[{request_id}] Multiple results but 1st stage said no clarification, invoking 2nd stage check...")

                # ê²°ê³¼ ìš”ì•½ ìƒì„±
                result_summaries = []
                for msg_idx, msg in result_messages:
                    entity = msg.queryPlan.get("entity", "unknown") if msg.queryPlan else "unknown"
                    count = 0
                    if msg.queryResult:
                        if isinstance(msg.queryResult, dict):
                            count = msg.queryResult.get("totalCount", msg.queryResult.get("metadata", {}).get("rowsReturned", 0))
                    filters_str = ""
                    if msg.queryPlan and msg.queryPlan.get("filters"):
                        filters_str = ", ".join([f"{f.get('field')}={f.get('value')}" for f in msg.queryPlan.get("filters", [])[:2]])
                    result_summaries.append({"entity": entity, "count": count, "filters": filters_str})

                # 2ë‹¨ê³„ LLM íŒë‹¨ í˜¸ì¶œ
                needs_result_clarification = await query_planner.check_clarification_needed(
                    user_message=request.message,
                    result_summaries=result_summaries,
                    query_intent=query_intent
                )
                logger.info(f"[{request_id}] 2nd stage LLM decision: needs_result_clarification={needs_result_clarification}")

            if len(result_messages) > 1 and needs_result_clarification:
                # ë‹¤ì¤‘ ê²°ê³¼ + LLMì´ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨: clarification ìš”ì²­
                recent_results = result_messages[-5:]  # ìµœê·¼ 5ê°œë§Œ
                options = []
                indices = []

                for idx, (msg_idx, msg) in enumerate(reversed(recent_results)):
                    # ê²°ê³¼ ìš”ì•½ ë¼ë²¨ ìƒì„±
                    entity = msg.queryPlan.get("entity", "ë°ì´í„°") if msg.queryPlan else "ë°ì´í„°"
                    count = "?"
                    if msg.queryResult:
                        if isinstance(msg.queryResult, dict):
                            count = msg.queryResult.get("totalCount", msg.queryResult.get("metadata", {}).get("rowsReturned", "?"))
                        elif hasattr(msg.queryResult, "totalCount"):
                            count = msg.queryResult.totalCount
                    time_str = msg.timestamp[-8:-3] if msg.timestamp and len(msg.timestamp) >= 8 else ""

                    label = f"ì§ì „: {entity} {count}ê±´ ({time_str})" if idx == 0 else f"{entity} {count}ê±´ ({time_str})"
                    options.append(label)
                    indices.append(msg_idx)

                logger.info(f"[{request_id}] Multiple results found, requesting clarification")

                clarification_render_spec = {
                    "type": "clarification",
                    "clarification": {
                        "question": "ì–´ë–¤ ì¡°íšŒ ê²°ê³¼ë¥¼ í•„í„°ë§í• ê¹Œìš”?",
                        "options": options
                    },
                    "metadata": {
                        "requestId": request_id,
                        "targetResultIndices": indices,
                        "pendingFilters": query_plan.get("filters", []),
                        "generatedAt": datetime.utcnow().isoformat() + "Z"
                    }
                }

                return ChatResponse(
                    request_id=request_id,
                    render_spec=clarification_render_spec,
                    query_result=None,
                    query_plan={**query_plan, "needs_clarification": True, "requestId": request_id},
                    ai_message="ì–´ë–¤ ì¡°íšŒ ê²°ê³¼ë¥¼ í•„í„°ë§í• ê¹Œìš”?",
                    timestamp=datetime.utcnow().isoformat() + "Z"
                )

            # ê²°ê³¼ê°€ 1ê°œ ì´í•˜: í´ë¼ì´ì–¸íŠ¸ì—ì„œ í•„í„°ë§í•˜ë„ë¡ ì‘ë‹µ
            target_index = result_messages[-1][0] if result_messages else -1
            logger.info(f"[{request_id}] Single result, returning filter_local response (target: {target_index})")

            filter_local_render_spec = {
                "type": "filter_local",
                "filter": query_plan.get("filters", []),
                "targetResultIndex": target_index,
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z"
                }
            }

            return ChatResponse(
                request_id=request_id,
                render_spec=filter_local_render_spec,
                query_result=None,
                query_plan={**query_plan, "requestId": request_id},
                ai_message="ì´ì „ ê²°ê³¼ì—ì„œ í•„í„°ë§í•©ë‹ˆë‹¤.",
                timestamp=datetime.utcnow().isoformat() + "Z"
            )
        elif query_intent == "aggregate_local":
            # í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ì§‘ê³„: ì´ì „ ê²°ê³¼ì—ì„œ ì§‘ê³„
            logger.info(f"[{request_id}] Intent: aggregate_local, client-side aggregation")

            # entityê°€ ì—†ìœ¼ë©´ ì´ì „ queryPlanì—ì„œ ìƒì†
            if not query_plan.get("entity") and request.conversation_history:
                previous_plan = get_previous_query_plan(request.conversation_history)
                if previous_plan and previous_plan.get("entity"):
                    query_plan["entity"] = previous_plan["entity"]
                    logger.info(f"[{request_id}] Inherited entity from previous plan: {query_plan['entity']}")

            # ì´ì „ ê²°ê³¼ê°€ ìˆëŠ” ë©”ì‹œì§€ë“¤ ì°¾ê¸°
            result_messages = []
            if request.conversation_history:
                logger.info(f"[{request_id}] Checking {len(request.conversation_history)} messages in history")
                for i, msg in enumerate(request.conversation_history):
                    has_query_result = msg.queryResult is not None
                    if msg.role == "assistant" and msg.queryResult:
                        result_messages.append((i, msg))

            logger.info(f"[{request_id}] Found {len(result_messages)} result messages for aggregation")

            # ì§‘ê³„ ì •ë³´ ì¶”ì¶œ
            aggregations = query_plan.get("aggregations", [])
            if not aggregations:
                # ê¸°ë³¸ ì§‘ê³„: sum(amount)
                aggregations = [{"function": "sum", "field": "amount", "alias": "totalAmount", "displayLabel": "ê²°ì œ ê¸ˆì•¡ í•©ê³„", "currency": "USD"}]

            # 1ë‹¨ê³„: LLMì´ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨í–ˆëŠ”ì§€ í™•ì¸
            needs_result_clarification = query_plan.get("needs_result_clarification", False)
            logger.info(f"[{request_id}] 1st stage LLM decision (aggregate): needs_result_clarification={needs_result_clarification}")

            # 2ë‹¨ê³„: ë‹¤ì¤‘ ê²°ê³¼ + 1ë‹¨ê³„ê°€ Falseë©´ ìƒìœ„ ëª¨ë¸ë¡œ ì¬íŒë‹¨
            if len(result_messages) > 1 and not needs_result_clarification:
                logger.info(f"[{request_id}] Multiple results but 1st stage said no clarification, invoking 2nd stage check...")

                # ê²°ê³¼ ìš”ì•½ ìƒì„±
                result_summaries = []
                for msg_idx, msg in result_messages:
                    entity = msg.queryPlan.get("entity", "unknown") if msg.queryPlan else "unknown"
                    count = 0
                    if msg.queryResult:
                        if isinstance(msg.queryResult, dict):
                            count = msg.queryResult.get("totalCount", msg.queryResult.get("metadata", {}).get("rowsReturned", 0))
                    filters_str = ""
                    if msg.queryPlan and msg.queryPlan.get("filters"):
                        filters_str = ", ".join([f"{f.get('field')}={f.get('value')}" for f in msg.queryPlan.get("filters", [])[:2]])
                    result_summaries.append({"entity": entity, "count": count, "filters": filters_str})

                # 2ë‹¨ê³„ LLM íŒë‹¨ í˜¸ì¶œ
                needs_result_clarification = await query_planner.check_clarification_needed(
                    user_message=request.message,
                    result_summaries=result_summaries,
                    query_intent=query_intent
                )
                logger.info(f"[{request_id}] 2nd stage LLM decision (aggregate): needs_result_clarification={needs_result_clarification}")

            if len(result_messages) > 1 and needs_result_clarification:
                # ë‹¤ì¤‘ ê²°ê³¼ + LLMì´ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨: clarification ìš”ì²­
                recent_results = result_messages[-5:]  # ìµœê·¼ 5ê°œë§Œ
                options = []
                indices = []

                for idx, (msg_idx, msg) in enumerate(reversed(recent_results)):
                    entity = msg.queryPlan.get("entity", "ë°ì´í„°") if msg.queryPlan else "ë°ì´í„°"
                    count = "?"
                    if msg.queryResult:
                        if isinstance(msg.queryResult, dict):
                            count = msg.queryResult.get("totalCount", msg.queryResult.get("metadata", {}).get("rowsReturned", "?"))
                    time_str = msg.timestamp[-8:-3] if msg.timestamp and len(msg.timestamp) >= 8 else ""

                    label = f"ì§ì „: {entity} {count}ê±´ ({time_str})" if idx == 0 else f"{entity} {count}ê±´ ({time_str})"
                    options.append(label)
                    indices.append(msg_idx)

                logger.info(f"[{request_id}] Multiple results found, requesting clarification for aggregation")

                clarification_render_spec = {
                    "type": "clarification",
                    "clarification": {
                        "question": "ì–´ë–¤ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í• ê¹Œìš”?",
                        "options": options
                    },
                    "metadata": {
                        "requestId": request_id,
                        "targetResultIndices": indices,
                        "pendingAggregations": aggregations,
                        "aggregationType": "aggregate_local",
                        "generatedAt": datetime.utcnow().isoformat() + "Z"
                    }
                }

                return ChatResponse(
                    request_id=request_id,
                    render_spec=clarification_render_spec,
                    query_result=None,
                    query_plan={**query_plan, "needs_clarification": True, "requestId": request_id},
                    ai_message="ì–´ë–¤ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í• ê¹Œìš”?",
                    timestamp=datetime.utcnow().isoformat() + "Z"
                )

            # ê²°ê³¼ê°€ 1ê°œ ì´í•˜: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì§‘ê³„í•˜ë„ë¡ ì‘ë‹µ
            target_index = result_messages[-1][0] if result_messages else -1
            logger.info(f"[{request_id}] Single result, returning aggregate_local response (target: {target_index})")

            aggregate_local_render_spec = {
                "type": "aggregate_local",
                "aggregations": aggregations,
                "targetResultIndex": target_index,
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z"
                }
            }

            return ChatResponse(
                request_id=request_id,
                render_spec=aggregate_local_render_spec,
                query_result=None,
                query_plan={**query_plan, "requestId": request_id},
                ai_message="ì´ì „ ê²°ê³¼ì—ì„œ ì§‘ê³„í•©ë‹ˆë‹¤.",
                timestamp=datetime.utcnow().isoformat() + "Z"
            )
        elif query_intent == "direct_answer":
            # LLMì´ ì§ì ‘ ë‹µë³€: DB ì¡°íšŒ ì—†ì´ í…ìŠ¤íŠ¸ ì‘ë‹µ
            direct_answer = query_plan.get("direct_answer", "")
            logger.info(f"[{request_id}] Intent: direct_answer, returning LLM response")

            if not direct_answer:
                direct_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            direct_answer_render_spec = {
                "type": "text",
                "title": "ë¶„ì„ ê²°ê³¼",
                "text": {
                    "content": direct_answer,
                    "format": "markdown"
                },
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z"
                }
            }

            return ChatResponse(
                request_id=request_id,
                render_spec=direct_answer_render_spec,
                query_result=None,
                query_plan={**query_plan, "requestId": request_id},
                ai_message=direct_answer,
                timestamp=datetime.utcnow().isoformat() + "Z"
            )
        else:
            logger.info(f"[{request_id}] Intent: new_query, no filter merge")

        query_plan["requestId"] = request_id

        processing_info["stages"].append({
            "name": "query_plan_generation",
            "durationMs": int((datetime.utcnow() - stage_start).total_seconds() * 1000),
            "status": "success"
        })

        logger.info(f"[{request_id}] Final QueryPlan: {query_plan}")

        # Clarification í•„ìš” ì‹œ ì¿¼ë¦¬ ì‹¤í–‰ ì—†ì´ ëŒ€í™”í˜• ì§ˆë¬¸ ë°˜í™˜
        if query_plan.get("needs_clarification"):
            question = query_plan.get("clarification_question", "ì–´ë–¤ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
            logger.info(f"[{request_id}] Clarification needed: {question}")

            # ëŒ€í™”í˜• í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µ (ë²„íŠ¼ ì—†ì´)
            clarification_render_spec = {
                "type": "text",
                "title": "ì¶”ê°€ ì •ë³´ í•„ìš”",
                "text": {
                    "content": question,
                    "format": "plain"
                },
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z"
                }
            }

            clarification_query_result = {
                "requestId": request_id,
                "status": "pending",
                "data": {"rows": [], "aggregations": {}},
                "metadata": {
                    "executionTimeMs": int((datetime.utcnow() - start_time).total_seconds() * 1000),
                    "rowsReturned": 0,
                    "dataSource": "clarification_required"
                }
            }

            return ChatResponse(
                request_id=request_id,
                render_spec=clarification_render_spec,
                query_result=clarification_query_result,
                query_plan=query_plan,
                ai_message=question,
                timestamp=datetime.utcnow().isoformat() + "Z"
            )

        # Stage 2: Call Core API
        stage_start = datetime.utcnow()
        query_result = await call_core_api(query_plan)

        processing_info["stages"].append({
            "name": "core_api_call",
            "durationMs": int((datetime.utcnow() - stage_start).total_seconds() * 1000),
            "status": "success" if query_result.get("status") == "success" else "error"
        })

        logger.info(f"[{request_id}] Core API response status: {query_result.get('status')}")

        # Stage 3: QueryResult â†’ RenderSpec
        stage_start = datetime.utcnow()
        render_composer = get_render_composer()
        render_spec = render_composer.compose(query_result, query_plan, request.message)

        processing_info["stages"].append({
            "name": "render_spec_composition",
            "durationMs": int((datetime.utcnow() - stage_start).total_seconds() * 1000),
            "status": "success"
        })

        # Calculate total processing time
        total_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        processing_info["totalDurationMs"] = total_time

        logger.info(f"[{request_id}] Completed in {total_time}ms")

        return ChatResponse(
            request_id=request_id,
            render_spec=render_spec,
            query_result=query_result,
            query_plan=query_plan,  # í›„ì† ì§ˆë¬¸ì—ì„œ ì´ì „ ì¿¼ë¦¬ ì¡°ê±´ ì°¸ì¡°ìš©
            ai_message=f"'{request.message}'ì— ëŒ€í•œ ê²°ê³¼ì…ë‹ˆë‹¤.",
            timestamp=datetime.utcnow().isoformat() + "Z"
        )

    except Exception as e:
        logger.error(f"[{request_id}] Error: {e}", exc_info=True)

        total_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        processing_info["totalDurationMs"] = total_time
        processing_info["error"] = str(e)

        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ RenderSpec ë°˜í™˜ (ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œìš©)
        error_render_spec = {
            "type": "text",
            "title": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "text": {
                "content": f"## ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤\n\n"
                          f"**ìš”ì²­**: {request.message}\n\n"
                          f"**ì˜¤ë¥˜**: {str(e)}\n\n"
                          f"ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "format": "markdown",
                "sections": [
                    {
                        "type": "error",
                        "title": "ì˜¤ë¥˜ ì •ë³´",
                        "content": str(e)
                    }
                ]
            },
            "metadata": {
                "requestId": request_id,
                "generatedAt": datetime.utcnow().isoformat() + "Z"
            }
        }

        # ì—ëŸ¬ ì‹œ ë¹ˆ QueryResult ë°˜í™˜
        error_query_result = {
            "requestId": request_id,
            "status": "error",
            "data": {"rows": [], "aggregations": {}},
            "metadata": {
                "executionTimeMs": total_time,
                "rowsReturned": 0,
                "totalRows": 0,
                "dataSource": "error"
            },
            "error": {
                "code": "PROCESSING_ERROR",
                "message": str(e)
            }
        }

        # ì—ëŸ¬ ì‹œ ë¹ˆ QueryPlan ë°˜í™˜
        error_query_plan = {
            "entity": "",
            "operation": "list"
        }

        return ChatResponse(
            request_id=request_id,
            render_spec=error_render_spec,
            query_result=error_query_result,
            query_plan=error_query_plan,
            ai_message=f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            timestamp=datetime.utcnow().isoformat() + "Z"
        )


async def call_core_api(query_plan: Dict[str, Any]) -> Dict[str, Any]:
    """Core API í˜¸ì¶œ"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{CORE_API_URL}/api/v1/query/start",
                json=query_plan
            )

            # HTTP ì—ëŸ¬ê°€ ì•„ë‹Œ ë¹„ì¦ˆë‹ˆìŠ¤ ì—ëŸ¬ë„ ì²˜ë¦¬
            if response.status_code >= 400:
                error_body = response.json() if response.headers.get("content-type", "").startswith("application/json") else {}
                logger.warning(f"Core API returned {response.status_code}: {error_body}")
                return error_body if error_body else {
                    "status": "error",
                    "error": {
                        "code": f"HTTP_{response.status_code}",
                        "message": response.text
                    }
                }

            return response.json()

    except httpx.TimeoutException:
        logger.error("Core API timeout")
        return {
            "status": "error",
            "error": {
                "code": "TIMEOUT",
                "message": "Core API ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
            }
        }
    except httpx.HTTPError as e:
        logger.error(f"Core API HTTP error: {e}")
        return {
            "status": "error",
            "error": {
                "code": "CONNECTION_ERROR",
                "message": f"Core API ì—°ê²° ì˜¤ë¥˜: {str(e)}"
            }
        }


@router.get("/chat/test")
async def test_core_api():
    """Core API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{CORE_API_URL}/api/v1/query/health")
            response.raise_for_status()
            return {
                "core_api_status": "reachable",
                "core_api_response": response.json()
            }
    except httpx.HTTPError as e:
        return {
            "core_api_status": "unreachable",
            "error": str(e)
        }


@router.get("/chat/config")
async def get_config():
    """í˜„ì¬ ì„¤ì • í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    return {
        "core_api_url": CORE_API_URL,
        "openai_api_key_set": bool(os.getenv("OPENAI_API_KEY")),
        "anthropic_api_key_set": bool(os.getenv("ANTHROPIC_API_KEY")),
        "rag_enabled": os.getenv("RAG_ENABLED", "true").lower() == "true",
        "database_url_set": bool(os.getenv("DATABASE_URL")),
        "query_plan_validation": {
            "enabled": ENABLE_QUERY_PLAN_VALIDATION,
            "validator_provider": os.getenv("VALIDATOR_LLM_PROVIDER", "openai"),
            "validator_model": os.getenv("VALIDATOR_LLM_MODEL", "gpt-4o-mini"),
            "quality_threshold": float(os.getenv("VALIDATOR_QUALITY_THRESHOLD", "0.8")),
            "use_llm_validation": os.getenv("VALIDATOR_USE_LLM", "true").lower() == "true"
        },
        "step": "8-query-plan-validation"
    }


@router.get("/chat/rag/status")
async def get_rag_status():
    """RAG ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        rag_service = get_rag_service()
        doc_counts = await rag_service.get_document_count()

        return {
            "status": "available",
            "rag_enabled": os.getenv("RAG_ENABLED", "true").lower() == "true",
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "document_counts": doc_counts,
            "total_documents": sum(doc_counts.values()) if doc_counts else 0
        }
    except Exception as e:
        return {
            "status": "unavailable",
            "error": str(e)
        }


@router.post("/chat/rag/search")
async def search_documents(query: str, k: int = 3):
    """RAG ë¬¸ì„œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        rag_service = get_rag_service()
        documents = await rag_service.search_docs(query=query, k=k)

        return {
            "query": query,
            "count": len(documents),
            "documents": [
                {
                    "id": doc.id,
                    "doc_type": doc.doc_type,
                    "title": doc.title,
                    "content": doc.content[:200] + "..." if len(doc.content) > 200 else doc.content,
                    "similarity": doc.similarity
                }
                for doc in documents
            ]
        }
    except Exception as e:
        return {
            "query": query,
            "error": str(e)
        }


# ============================================
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
# ============================================

class DownloadRequest(BaseModel):
    """ë‹¤ìš´ë¡œë“œ ìš”ì²­"""
    sql: str
    format: str = "csv"  # csv ë˜ëŠ” excel


@router.post("/chat/download")
async def download_query_result(request: DownloadRequest):
    """
    ëŒ€ìš©ëŸ‰ ì¿¼ë¦¬ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ

    - SQL ì¬ê²€ì¦ í›„ ì‹¤í–‰ (LIMIT ì—†ì´)
    - Streaming ì‘ë‹µìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
    """
    if not ENABLE_TEXT_TO_SQL:
        raise HTTPException(400, "Text-to-SQL mode is not enabled")

    from app.services.sql_validator import get_sql_validator
    from app.services.text_to_sql import get_text_to_sql_service

    # SQL ê²€ì¦ (ë³´ì•ˆ)
    validator = get_sql_validator()
    validation = validator.validate(request.sql)

    if not validation.is_valid:
        raise HTTPException(400, f"Invalid SQL: {', '.join(validation.issues)}")

    # LIMIT ì œê±° (ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ)
    unlimited_sql = re.sub(r'\bLIMIT\s+\d+', '', validation.sanitized_sql, flags=re.IGNORECASE)
    unlimited_sql = re.sub(r'\bOFFSET\s+\d+', '', unlimited_sql, flags=re.IGNORECASE)

    logger.info(f"Download request - Original SQL: {request.sql[:100]}...")
    logger.info(f"Download request - Unlimited SQL: {unlimited_sql[:100]}...")

    text_to_sql = get_text_to_sql_service()

    def generate_csv() -> Generator[str, None, None]:
        """CSV ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ê¸°"""
        import psycopg
        from psycopg.rows import dict_row

        try:
            with text_to_sql._get_readonly_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(unlimited_sql)

                    # í—¤ë” ì¶œë ¥
                    columns = [desc[0] for desc in cur.description]
                    output = io.StringIO()
                    writer = csv.writer(output)
                    writer.writerow(columns)
                    yield output.getvalue()

                    # ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ (1000ê±´ì”©)
                    batch_size = 1000
                    row_count = 0
                    while True:
                        rows = cur.fetchmany(batch_size)
                        if not rows:
                            break

                        output = io.StringIO()
                        writer = csv.writer(output)
                        for row in rows:
                            # datetime ë³€í™˜
                            processed_row = []
                            for value in row.values() if hasattr(row, 'values') else row:
                                if hasattr(value, 'isoformat'):
                                    processed_row.append(value.isoformat())
                                else:
                                    processed_row.append(value)
                            writer.writerow(processed_row)
                            row_count += 1

                        yield output.getvalue()

                    logger.info(f"Download completed: {row_count} rows")

        except psycopg.Error as e:
            logger.error(f"Download SQL execution failed: {e}")
            yield f"Error: {str(e)}"

    def generate_excel() -> bytes:
        """Excel íŒŒì¼ ìƒì„± (ë©”ëª¨ë¦¬ ë‚´)"""
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment
        import psycopg

        wb = Workbook()
        ws = wb.active
        ws.title = "Query Result"

        try:
            with text_to_sql._get_readonly_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(unlimited_sql)

                    # í—¤ë” í–‰ ìŠ¤íƒ€ì¼
                    columns = [desc[0] for desc in cur.description]
                    header_font = Font(bold=True, color="FFFFFF")
                    header_fill = PatternFill(start_color="4F81BD", end_color="4F81BD", fill_type="solid")

                    for col_idx, col_name in enumerate(columns, 1):
                        cell = ws.cell(row=1, column=col_idx, value=col_name)
                        cell.font = header_font
                        cell.fill = header_fill
                        cell.alignment = Alignment(horizontal="center")

                    # ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ (1000ê±´ì”©)
                    batch_size = 1000
                    row_num = 2
                    while True:
                        rows = cur.fetchmany(batch_size)
                        if not rows:
                            break

                        for row in rows:
                            values = row.values() if hasattr(row, 'values') else row
                            for col_idx, value in enumerate(values, 1):
                                # datetimeì„ ISO format ë¬¸ìì—´ë¡œ ë³€í™˜
                                if hasattr(value, 'isoformat'):
                                    value = value.isoformat()
                                # dict/list (JSONB)ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                                elif isinstance(value, (dict, list)):
                                    import json
                                    value = json.dumps(value, ensure_ascii=False, default=str)
                                ws.cell(row=row_num, column=col_idx, value=value)
                            row_num += 1

                    logger.info(f"Excel generation completed: {row_num - 2} rows")

        except psycopg.Error as e:
            logger.error(f"Excel SQL execution failed: {e}")
            raise HTTPException(500, f"Excel generation failed: {str(e)}")

        # BytesIOë¡œ ì €ì¥
        output = io.BytesIO()
        wb.save(output)
        output.seek(0)
        return output.getvalue()

    # íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")

    # formatì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬
    if request.format == "excel":
        try:
            excel_data = generate_excel()
            filename = f"query_result_{timestamp}.xlsx"

            from fastapi.responses import Response
            return Response(
                content=excel_data,
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={
                    "Content-Disposition": f"attachment; filename={filename}",
                    "X-Content-Type-Options": "nosniff"
                }
            )
        except Exception as e:
            logger.error(f"Excel download failed: {e}")
            raise HTTPException(500, f"Excel generation failed: {str(e)}")

    # CSV ì‘ë‹µ (ê¸°ë³¸)
    filename = f"query_result_{timestamp}.csv"
    return StreamingResponse(
        generate_csv(),
        media_type="text/csv; charset=utf-8",
        headers={
            "Content-Disposition": f"attachment; filename={filename}",
            "X-Content-Type-Options": "nosniff"
        }
    )


# ============================================
# Text-to-SQL ëª¨ë“œ í•¸ë“¤ëŸ¬
# ============================================

async def handle_text_to_sql(
    request: ChatRequest,
    request_id: str,
    start_time: datetime
) -> ChatResponse:
    """
    Text-to-SQL ëª¨ë“œ ì²˜ë¦¬

    AIê°€ ì§ì ‘ SQLì„ ìƒì„±í•˜ê³  ì½ê¸° ì „ìš© DBì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger.info(f"[{request_id}] Text-to-SQL mode: processing")

    try:
        text_to_sql = get_text_to_sql_service()

        # ì°¸ì¡° í‘œí˜„ ê°ì§€ (ì—°ì† ëŒ€í™” WHERE ì¡°ê±´ ë³‘í•©ìš©)
        is_refinement, ref_type = detect_reference_expression(request.message)
        if is_refinement:
            logger.info(f"[{request_id}] Reference expression detected (type: {ref_type}), will preserve previous WHERE conditions")

        # ëŒ€í™” ì´ë ¥ ë³€í™˜ (Text-to-SQL í˜•ì‹)
        sql_history = build_sql_history(request.conversation_history)

        # SQL ìƒì„± ë° ì‹¤í–‰ (is_refinement ì „ë‹¬)
        result = await text_to_sql.query(
            question=request.message,
            conversation_history=sql_history,
            retry_on_error=True,
            is_refinement=is_refinement
        )

        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        total_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

        if result["success"]:
            # ì„±ê³µ: ë°ì´í„°ë¥¼ RenderSpecìœ¼ë¡œ ë³€í™˜
            # LLM ì¶”ì²œ ì°¨íŠ¸ íƒ€ì… ë° ì¸ì‚¬ì´íŠ¸ í…œí”Œë¦¿ ì¶”ì¶œ
            llm_chart_type = result.get("llmChartType")
            insight_template = result.get("insightTemplate")
            if llm_chart_type:
                logger.info(f"[{request_id}] LLM chart type: {llm_chart_type}")
            if insight_template:
                logger.info(f"[{request_id}] LLM insight template: {insight_template[:50]}...")

            render_spec = compose_sql_render_spec(result, request.message, llm_chart_type, insight_template)

            # ì§‘ê³„ ì¿¼ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            is_aggregation = result.get("isAggregation", False)
            aggregation_context = result.get("aggregationContext")

            query_result = {
                "requestId": request_id,
                "status": "success",
                "data": {
                    "rows": result["data"],
                    "aggregations": {}
                },
                "metadata": {
                    "executionTimeMs": int(result.get("executionTimeMs") or 0),
                    "rowsReturned": result["rowCount"],
                    "totalRows": result["rowCount"],
                    "dataSource": "text_to_sql"
                },
                # ì§‘ê³„ ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                "isAggregation": is_aggregation,
                "aggregationContext": aggregation_context
            }
        else:
            # ì‹¤íŒ¨: ì—ëŸ¬ RenderSpec
            render_spec = {
                "type": "text",
                "title": "ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜",
                "text": {
                    "content": f"## ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤\n\n"
                              f"**ì§ˆë¬¸**: {request.message}\n\n"
                              f"**ì˜¤ë¥˜**: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n\n"
                              f"**ìƒì„±ëœ SQL**:\n```sql\n{result.get('sql', 'N/A')}\n```",
                    "format": "markdown"
                },
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z",
                    "mode": "text_to_sql"
                }
            }
            query_result = {
                "requestId": request_id,
                "status": "error",
                "data": {"rows": [], "aggregations": {}},
                "metadata": {
                    "executionTimeMs": total_time_ms,
                    "rowsReturned": 0,
                    "dataSource": "text_to_sql"
                },
                "error": {
                    "code": "SQL_EXECUTION_ERROR",
                    "message": result.get("error", "Unknown error")
                }
            }

        logger.info(f"[{request_id}] Text-to-SQL completed: success={result['success']}, rows={result['rowCount']}")

        return ChatResponse(
            request_id=request_id,
            render_spec=render_spec,
            query_result=query_result,
            query_plan={
                "mode": "text_to_sql",
                "sql": result.get("sql"),
                "requestId": request_id
            },
            ai_message=f"'{request.message}'ì— ëŒ€í•œ ê²°ê³¼ì…ë‹ˆë‹¤." if result["success"] else "ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            timestamp=datetime.utcnow().isoformat() + "Z"
        )

    except Exception as e:
        logger.error(f"[{request_id}] Text-to-SQL error: {e}", exc_info=True)
        total_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

        error_render_spec = {
            "type": "text",
            "title": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "text": {
                "content": f"## Text-to-SQL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤\n\n"
                          f"**ìš”ì²­**: {request.message}\n\n"
                          f"**ì˜¤ë¥˜**: {str(e)}\n\n"
                          f"ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "format": "markdown"
            },
            "metadata": {
                "requestId": request_id,
                "generatedAt": datetime.utcnow().isoformat() + "Z",
                "mode": "text_to_sql"
            }
        }

        return ChatResponse(
            request_id=request_id,
            render_spec=error_render_spec,
            query_result={
                "requestId": request_id,
                "status": "error",
                "data": {"rows": [], "aggregations": {}},
                "metadata": {"executionTimeMs": total_time_ms, "rowsReturned": 0}
            },
            query_plan={"mode": "text_to_sql", "error": str(e)},
            ai_message=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            timestamp=datetime.utcnow().isoformat() + "Z"
        )


def build_sql_history(conversation_history: Optional[List[ChatMessageItem]]) -> List[Dict[str, Any]]:
    """
    ëŒ€í™” ì´ë ¥ì„ Text-to-SQL í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    ëŒ€í™” ê¸°ë°˜ ë§¥ë½ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨:
    - role: ë©”ì‹œì§€ ì—­í•  (user/assistant)
    - content: ë©”ì‹œì§€ ë‚´ìš©
    - sql: ìƒì„±ëœ SQL (assistant ë©”ì‹œì§€)
    - rowCount: ì¿¼ë¦¬ ê²°ê³¼ ê±´ìˆ˜ (assistant ë©”ì‹œì§€)
    - whereConditions: WHERE ì¡°ê±´ ëª©ë¡ (assistant ë©”ì‹œì§€) - Phase 1: ëª…ì‹œì  ì €ì¥
    """
    if not conversation_history:
        return []

    sql_history = []
    for msg in conversation_history[-10:]:  # ìµœê·¼ 10ê°œë§Œ
        entry: Dict[str, Any] = {
            "role": msg.role,
            "content": msg.content
        }

        # assistant ë©”ì‹œì§€ì— SQL ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨
        if msg.role == "assistant" and msg.queryPlan:
            if msg.queryPlan.get("mode") == "text_to_sql" and msg.queryPlan.get("sql"):
                sql = msg.queryPlan.get("sql")
                entry["sql"] = sql

                # Phase 1: WHERE ì¡°ê±´ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ì €ì¥
                # ì´ë¥¼ í†µí•´ 4ë‹¨ê³„+ ì²´ì´ë‹ì—ì„œë„ ì¡°ê±´ì´ ìœ ì‹¤ë˜ì§€ ì•ŠìŒ
                if ENABLE_TEXT_TO_SQL:
                    where_conditions = extract_where_conditions(sql)
                    if where_conditions:
                        entry["whereConditions"] = where_conditions

        # ê²°ê³¼ ê±´ìˆ˜ ì¶”ì¶œ (queryResultì˜ metadataì—ì„œ)
        if msg.role == "assistant" and msg.queryResult:
            metadata = msg.queryResult.get("metadata", {})
            # totalRows ë˜ëŠ” rowsReturned ìš°ì„ ìˆœìœ„ë¡œ í™•ì¸
            row_count = (
                msg.queryResult.get("totalCount") or
                metadata.get("totalRows") or
                metadata.get("rowsReturned")
            )
            if row_count is not None:
                entry["rowCount"] = row_count

        sql_history.append(entry)

    return sql_history


# ============================================
# ì°¨íŠ¸ ë Œë”ë§ ê°ì§€ ë° êµ¬ì„± (TC-001)
# ============================================
# NOTE: CHART_KEYWORDS, TABLE_KEYWORDS, TIME_FIELD_KEYWORDSëŠ”
#       app.constants.render_keywordsì—ì„œ importë¨


def _detect_render_type_from_message(message: str) -> Optional[str]:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë Œë”ë§ íƒ€ì… ê°ì§€

    ìƒìˆ˜ íŒŒì¼(render_keywords.py)ì—ì„œ í‚¤ì›Œë“œë¥¼ importí•˜ì—¬ ì‚¬ìš©

    ìš°ì„ ìˆœìœ„:
    1. í…Œì´ë¸” í‚¤ì›Œë“œ ("ê·¸ë˜í”„ ë§ê³  í‘œë¡œ" ê°™ì€ ë¶€ì • í‘œí˜„ ì²˜ë¦¬)
    2. ì°¨íŠ¸ í‚¤ì›Œë“œ (ë‹¨ë… í‚¤ì›Œë“œ "ê·¸ë˜í”„", "ì°¨íŠ¸" í¬í•¨)

    Args:
        message: ì‚¬ìš©ì ì§ˆë¬¸

    Returns:
        "chart" | "table" | None
    """
    msg = message.lower()

    # 1ìˆœìœ„: í…Œì´ë¸” í‚¤ì›Œë“œ ê°ì§€ (ë¶€ì • í‘œí˜„ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¨¼ì € ì²´í¬)
    if any(kw in msg for kw in TABLE_KEYWORDS):
        return "table"

    # 2ìˆœìœ„: ì°¨íŠ¸ í‚¤ì›Œë“œ ê°ì§€ (ë‹¨ë… í‚¤ì›Œë“œ í¬í•¨)
    if any(kw in msg for kw in CHART_KEYWORDS):
        return "chart"

    return None


def _detect_chart_type(data: List[Dict[str, Any]], columns: List[str], user_message: str = "") -> str:
    """ë°ì´í„° êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì°¨íŠ¸ íƒ€ì… ê²°ì • (í´ë°± ë¡œì§)

    LLM ê¸°ë°˜ ì°¨íŠ¸ íƒ€ì… ê²°ì • ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ í´ë°± ë¡œì§.
    ì‚¬ìš©ì ë©”ì‹œì§€ì˜ í‚¤ì›Œë“œì™€ ë°ì´í„° êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì°¨íŠ¸ íƒ€ì… ê²°ì •.

    Args:
        data: ì¿¼ë¦¬ ê²°ê³¼ ë°ì´í„°
        columns: ì»¬ëŸ¼ ëª©ë¡
        user_message: ì‚¬ìš©ì ì§ˆë¬¸ (í‚¤ì›Œë“œ ë¶„ì„ìš©)

    Returns:
        "bar" | "line" | "pie"
    """
    # render_keywordsì—ì„œ importëœ ìƒìˆ˜ ì‚¬ìš©
    from app.constants.render_keywords import CHART_TYPE_KEYWORDS, DATE_FIELDS

    if not data or not columns:
        return "bar"

    message_lower = user_message.lower()

    # ì‹œê³„ì—´ ì»¬ëŸ¼ ê°ì§€ (DATE_FIELDS ìƒìˆ˜ ì‚¬ìš© - camelCase, snake_case ëª¨ë‘ ì§€ì›)
    has_time_column = any(
        col.lower() in [f.lower() for f in DATE_FIELDS]
        or any(kw in col.lower() for kw in TIME_FIELD_KEYWORDS)
        for col in columns
    )

    # line í‚¤ì›Œë“œ ì²´í¬ (ì¶”ì´, ë³€í™”, íŠ¸ë Œë“œ ë“±)
    line_keywords = CHART_TYPE_KEYWORDS.get("line", [])
    has_line_keyword = any(kw in message_lower for kw in line_keywords)

    # ì‹œê³„ì—´ + line í‚¤ì›Œë“œ â†’ line (ë°ì´í„° í–‰ ìˆ˜ì™€ ë¬´ê´€)
    if has_time_column and has_line_keyword:
        logger.info(f"[ChartType Fallback] time_column + line_keyword â†’ line")
        return "line"

    # ì‹œê³„ì—´ + 2í–‰ ì´ìƒ â†’ line (ê¸°ì¡´ ì„ê³„ê°’ ì™„í™”: >2 â†’ >=2)
    if has_time_column and len(data) >= 2:
        logger.info(f"[ChartType Fallback] time_column + data>=2 â†’ line")
        return "line"

    # pie í‚¤ì›Œë“œ â†’ pie (10í–‰ ì´í•˜ì¼ ë•Œë§Œ)
    pie_keywords = CHART_TYPE_KEYWORDS.get("pie", [])
    if any(kw in message_lower for kw in pie_keywords) and len(data) <= 10:
        logger.info(f"[ChartType Fallback] pie_keyword + data<=10 â†’ pie")
        return "pie"

    # ì¹´í…Œê³ ë¦¬ê°€ ì ê³  (5ê°œ ì´í•˜) ë‹¨ì¼ ê°’ ì»¬ëŸ¼ì´ë©´ pie ì°¨íŠ¸
    # ë‹¨, line í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ
    if len(data) <= 5 and len(columns) == 2 and not has_line_keyword:
        logger.info(f"[ChartType Fallback] small_data + 2_cols + no_line_keyword â†’ pie")
        return "pie"

    # ê¸°ë³¸ì€ bar ì°¨íŠ¸
    logger.info(f"[ChartType Fallback] default â†’ bar")
    return "bar"


def _identify_axis_keys(data: List[Dict[str, Any]], columns: List[str]) -> Tuple[str, str]:
    """Xì¶•ê³¼ Yì¶•ì— ì‚¬ìš©í•  í‚¤ ì‹ë³„

    Args:
        data: ì¿¼ë¦¬ ê²°ê³¼ ë°ì´í„°
        columns: ì»¬ëŸ¼ ëª©ë¡

    Returns:
        (x_key, y_key) íŠœí”Œ
    """
    if not columns:
        return ("", "")

    if len(columns) == 1:
        return (columns[0], columns[0])

    # ìˆ«ìí˜• ì»¬ëŸ¼ ì°¾ê¸° (Yì¶• í›„ë³´)
    numeric_cols = []
    category_cols = []

    if data:
        first_row = data[0]
        for col in columns:
            value = first_row.get(col)
            if isinstance(value, (int, float)):
                numeric_cols.append(col)
            else:
                category_cols.append(col)

    # Xì¶•: ì¹´í…Œê³ ë¦¬/ì‹œê°„ ì»¬ëŸ¼, Yì¶•: ìˆ«ì ì»¬ëŸ¼
    x_key = category_cols[0] if category_cols else columns[0]
    y_key = numeric_cols[0] if numeric_cols else columns[-1]

    return (x_key, y_key)


def _detect_trend(values: List[float]) -> Optional[str]:
    """ì‹œê³„ì—´ ë°ì´í„°ì˜ ì¶”ì„¸ ê°ì§€

    Args:
        values: Yì¶• ê°’ ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ ìˆœì„œëŒ€ë¡œ)

    Returns:
        "ì¦ê°€" | "ê°ì†Œ" | "ìœ ì§€" | None (ë°ì´í„° ë¶€ì¡±ì‹œ)
    """
    if len(values) < 3:
        return None

    # ì „ë°˜ë¶€ì™€ í›„ë°˜ë¶€ì˜ í‰ê·  ë¹„êµ
    mid = len(values) // 2
    first_half = sum(values[:mid]) / mid
    second_half = sum(values[mid:]) / (len(values) - mid)

    if first_half == 0:
        return "ì¦ê°€" if second_half > 0 else "ìœ ì§€"

    diff_ratio = (second_half - first_half) / first_half

    if diff_ratio > 0.1:
        return "ì¦ê°€"
    elif diff_ratio < -0.1:
        return "ê°ì†Œ"
    return "ìœ ì§€"


def _generate_insight(
    data: List[Dict[str, Any]],
    x_key: str,
    y_key: str,
    chart_type: str,
    template: Optional[str] = None
) -> Dict[str, Any]:
    """ì°¨íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„± (LLM í…œí”Œë¦¿ ìš°ì„ , ê·œì¹™ ê¸°ë°˜ í´ë°±)

    Args:
        data: ì°¨íŠ¸ ë°ì´í„°
        x_key: Xì¶• í•„ë“œ í‚¤
        y_key: Yì¶• í•„ë“œ í‚¤
        chart_type: ì°¨íŠ¸ íƒ€ì… (line, bar, pie)
        template: LLMì´ ìƒì„±í•œ ì¸ì‚¬ì´íŠ¸ í…œí”Œë¦¿ (ì„ íƒì )

    Returns:
        {
            "content": "ì¸ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸",
            "source": "llm" | "template" | "none"
        }
    """
    if not data:
        return {"content": None, "source": "none"}

    # ìˆ«ì ê°’ ì¶”ì¶œ
    values = []
    for row in data:
        val = row.get(y_key, 0)
        if isinstance(val, (int, float)):
            values.append(float(val))
        elif val is not None:
            try:
                values.append(float(val))
            except (ValueError, TypeError):
                values.append(0)

    if not values:
        return {"content": None, "source": "none"}

    # í†µê³„ ê³„ì‚°
    count = len(data)
    total = sum(values)
    avg = total / count if count > 0 else 0
    max_val = max(values)
    min_val = min(values)

    # ìµœëŒ€/ìµœì†Œ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
    max_idx = values.index(max_val)
    min_idx = values.index(min_val)
    max_category = str(data[max_idx].get(x_key, ""))
    min_category = str(data[min_idx].get(x_key, ""))

    # ì¶”ì„¸ ê°ì§€ (line ì°¨íŠ¸ì—ì„œë§Œ)
    trend = _detect_trend(values) if chart_type == "line" else None

    # í•„ë“œ ë¼ë²¨ ë§¤í•‘ (snake_case â†’ í•œê¸€)
    FIELD_LABELS = {
        "month": "ì›”",
        "date": "ì¼",
        "week": "ì£¼",
        "year": "ì—°ë„",
        "day": "ì¼",
        "merchant_id": "ê°€ë§¹ì ",
        "status": "ìƒíƒœ",
        "method": "ê²°ì œìˆ˜ë‹¨",
        "amount": "ê¸ˆì•¡",
        "total_amount": "ë§¤ì¶œ",
        "sum_amount": "ì´ê¸ˆì•¡",
        "count": "ê±´ìˆ˜",
        "payment_count": "ê²°ì œê±´ìˆ˜",
        "refund_count": "í™˜ë¶ˆê±´ìˆ˜",
        "avg_amount": "í‰ê· ê¸ˆì•¡",
        "net_amount": "ì •ì‚°ê¸ˆì•¡",
        "total": "í•©ê³„",
        "avg": "í‰ê· ",
    }

    # í•„ë“œ ë¼ë²¨ ì¶”ì¶œ (snake_case, camelCase ëª¨ë‘ ì§€ì›)
    def get_field_label(key: str) -> str:
        key_lower = key.lower()
        if key_lower in FIELD_LABELS:
            return FIELD_LABELS[key_lower]
        # snake_case ì²˜ë¦¬
        parts = key.split('_')
        for part in parts:
            if part.lower() in FIELD_LABELS:
                return FIELD_LABELS[part.lower()]
        # ê¸°ë³¸ê°’: ê·¸ëŒ€ë¡œ ë°˜í™˜
        return key.replace('_', ' ').title()

    group_by_label = get_field_label(x_key)
    metric_label = get_field_label(y_key)

    # ê¸ˆì•¡ í¬ë§·íŒ… í•¨ìˆ˜
    def format_currency(val: float) -> str:
        if val >= 1000:
            return f"â‚©{int(val):,}"
        return f"{val:,.1f}"

    # í”Œë ˆì´ìŠ¤í™€ë” ê°’ êµ¬ì„±
    placeholders = {
        "{count}": f"{count:,}",
        "{total}": format_currency(total),
        "{avg}": format_currency(avg),
        "{max}": format_currency(max_val),
        "{min}": format_currency(min_val),
        "{maxCategory}": max_category,
        "{minCategory}": min_category,
        "{trend}": trend or "",
        "{groupBy}": group_by_label,
        "{metric}": metric_label,
    }

    # LLM í…œí”Œë¦¿ì´ ìˆìœ¼ë©´ í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜
    if template:
        content = template
        for placeholder, value in placeholders.items():
            content = content.replace(placeholder, value)

        # ë¯¸ì¹˜í™˜ í”Œë ˆì´ìŠ¤í™€ë” ì œê±° (ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª©)
        import re
        content = re.sub(r'\{[^}]+\}', '', content)
        content = re.sub(r'\s+', ' ', content).strip()

        logger.info(f"[Insight] Generated from LLM template: {content[:100]}...")
        return {"content": content, "source": "llm"}

    # í´ë°±: ê·œì¹™ ê¸°ë°˜ í…œí”Œë¦¿
    if chart_type == "line":
        content = f"{group_by_label}ë³„ {metric_label} ì¶”ì´ì…ë‹ˆë‹¤. ì´ {count:,}ê°œ ë°ì´í„°ì˜ í•©ê³„ëŠ” {format_currency(total)}ì…ë‹ˆë‹¤."
        if trend:
            content += f" ì „ë°˜ì ìœ¼ë¡œ {trend} ì¶”ì„¸ì…ë‹ˆë‹¤."
    elif chart_type == "pie":
        content = f"{group_by_label}ë³„ {metric_label} ë¶„í¬ì…ë‹ˆë‹¤. {max_category}ê°€ ê°€ì¥ í° ë¹„ì¤‘ì„ ì°¨ì§€í•©ë‹ˆë‹¤."
    else:  # bar
        content = f"{group_by_label}ë³„ {metric_label} ë¹„êµì…ë‹ˆë‹¤. {max_category}ê°€ {format_currency(max_val)}ë¡œ ê°€ì¥ ë†’ìŠµë‹ˆë‹¤."

    logger.info(f"[Insight] Generated from rule-based template: {content[:100]}...")
    return {"content": content, "source": "template"}


def _compose_chart_render_spec(
    result: Dict[str, Any],
    question: str,
    llm_chart_type: Optional[str] = None,
    insight_template: Optional[str] = None
) -> Dict[str, Any]:
    """ì°¨íŠ¸ íƒ€ì…ì˜ RenderSpec êµ¬ì„±

    Args:
        result: SQL ì‹¤í–‰ ê²°ê³¼
        question: ì‚¬ìš©ì ì§ˆë¬¸
        llm_chart_type: LLMì´ ì¶”ì²œí•œ ì°¨íŠ¸ íƒ€ì… (ìš°ì„  ì‚¬ìš©)
        insight_template: LLMì´ ìƒì„±í•œ ì¸ì‚¬ì´íŠ¸ í…œí”Œë¦¿ (ì„ íƒì )

    Returns:
        ì°¨íŠ¸ íƒ€ì… RenderSpec (insight í•„ë“œ í¬í•¨)
    """
    data = result.get("data", [])
    row_count = result.get("rowCount", 0)

    if not data:
        return {
            "type": "text",
            "title": "ì°¨íŠ¸ ìƒì„± ë¶ˆê°€",
            "text": {
                "content": "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ì–´ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "format": "plain"
            },
            "metadata": {
                "sql": result.get("sql"),
                "executionTimeMs": result.get("executionTimeMs"),
                "mode": "text_to_sql"
            }
        }

    columns = list(data[0].keys())

    # LLM ì¶”ì²œ ì°¨íŠ¸ íƒ€ì… ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê·œì¹™ ê¸°ë°˜ í´ë°±
    if llm_chart_type and llm_chart_type in ["line", "bar", "pie"]:
        chart_type = llm_chart_type
        logger.info(f"[ChartType] Using LLM recommendation: {chart_type}")
    else:
        # í´ë°±: ê·œì¹™ ê¸°ë°˜ ë¡œì§ (ê°œì„  ë²„ì „ - user_message ì „ë‹¬)
        chart_type = _detect_chart_type(data, columns, question)
        logger.info(f"[ChartType] Fallback to rule-based: {chart_type}")

    x_key, y_key = _identify_axis_keys(data, columns)

    # Xì¶• ë¼ë²¨ ìƒì„±
    x_label = x_key.replace("_", " ").title()
    y_label = y_key.replace("_", " ").title()

    # ì°¨íŠ¸ íƒ€ì…ë³„ ì œëª©
    chart_type_names = {
        "bar": "ë§‰ëŒ€ ê·¸ë˜í”„",
        "line": "ì¶”ì´ ê·¸ë˜í”„",
        "pie": "íŒŒì´ ì°¨íŠ¸"
    }
    title = f"{chart_type_names.get(chart_type, 'ì°¨íŠ¸')} ({row_count}ê±´)"

    render_spec = {
        "type": "chart",
        "title": title,
        "chart": {
            "chartType": chart_type,
            "dataRef": "data.rows",
            "xAxis": {
                "dataKey": x_key,
                "label": x_label,
                "type": "category" if chart_type != "line" else "time"
            },
            "yAxis": {
                "dataKey": y_key,
                "label": y_label,
                "type": "number"
            },
            "series": [
                {
                    "dataKey": y_key,
                    "name": y_label,
                    "type": chart_type if chart_type in ["bar", "line"] else "bar"
                }
            ],
            "legend": True,
            "tooltip": True
        },
        "data": data,
        "metadata": {
            "sql": result.get("sql"),
            "executionTimeMs": result.get("executionTimeMs"),
            "mode": "text_to_sql",
            "chartType": chart_type
        }
    }

    # pie ì°¨íŠ¸ì˜ ê²½ìš° series ëŒ€ì‹  ë³„ë„ ì„¤ì •
    if chart_type == "pie":
        render_spec["chart"]["series"] = [
            {
                "dataKey": y_key,
                "name": y_label
            }
        ]

    # ì¸ì‚¬ì´íŠ¸ ìƒì„± ë° ì¶”ê°€
    insight = _generate_insight(
        data=data,
        x_key=x_key,
        y_key=y_key,
        chart_type=chart_type,
        template=insight_template
    )
    render_spec["chart"]["insight"] = insight

    return render_spec


def _escape_markdown_table_cell(value: str) -> str:
    """Markdown í…Œì´ë¸” ì…€ì˜ íŠ¹ìˆ˜ë¬¸ì escape ì²˜ë¦¬

    í…Œì´ë¸”ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ íŒŒì´í”„(|), ë°±í‹±(`) ë“± ì²˜ë¦¬
    """
    if value is None:
        return "-"
    s = str(value)
    # íŒŒì´í”„ ë¬¸ìëŠ” í…Œì´ë¸” êµ¬ë¶„ìì™€ ì¶©ëŒí•˜ë¯€ë¡œ escape
    s = s.replace("|", "\\|")
    # ì¤„ë°”ê¿ˆì€ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
    s = s.replace("\n", " ").replace("\r", "")
    return s


def _format_aggregation_as_markdown_table(
    row: Dict[str, Any],
    aggregation_context: Optional[Dict[str, Any]] = None
) -> str:
    """ì§‘ê³„ ê²°ê³¼ë¥¼ Markdown í…Œì´ë¸”ë¡œ ë³€í™˜

    Args:
        row: ì§‘ê³„ ê²°ê³¼ ë‹¨ì¼ í–‰ (ì˜ˆ: {"total_amount": 14477000, "fee": 86862})
        aggregation_context: ì§‘ê³„ ì»¨í…ìŠ¤íŠ¸ (humanizedFilters í¬í•¨)

    Returns:
        Markdown í˜•ì‹ ë¬¸ìì—´
    """
    # ì»¬ëŸ¼ëª… â†’ í•œê¸€ ë¼ë²¨ ë§¤í•‘
    COLUMN_LABELS = {
        # SQL ì§‘ê³„ í•¨ìˆ˜ ê²°ê³¼ëª… (PostgreSQL ê¸°ë³¸ ë°˜í™˜ëª…)
        "sum": "í•©ê³„",
        "count": "ê±´ìˆ˜",
        "avg": "í‰ê· ",
        "max": "ìµœëŒ€ê°’",
        "min": "ìµœì†Œê°’",
        # ë³„ì¹­ì„ ê°€ì§„ ì§‘ê³„ ê²°ê³¼
        "original_amount": "ì›ê¸ˆì•¡",
        "fee": "ìˆ˜ìˆ˜ë£Œ",
        "amount_excluding_fee": "ìˆ˜ìˆ˜ë£Œ ì œì™¸ ê¸ˆì•¡",
        "total_amount": "ì´ ê¸ˆì•¡",
        "total_fee": "ì´ ìˆ˜ìˆ˜ë£Œ",
        "avg_amount": "í‰ê·  ê¸ˆì•¡",
        "average_amount": "í‰ê·  ê¸ˆì•¡",
        "max_amount": "ìµœëŒ€ ê¸ˆì•¡",
        "min_amount": "ìµœì†Œ ê¸ˆì•¡",
        "sum_amount": "í•©ê³„ ê¸ˆì•¡",
        "payment_count": "ê²°ì œ ê±´ìˆ˜",
        "refund_count": "í™˜ë¶ˆ ê±´ìˆ˜",
        "net_amount": "ì •ì‚° ê¸ˆì•¡",
        "total_payment_amount": "ì´ ê²°ì œ ê¸ˆì•¡",
        "total_refund_amount": "ì´ í™˜ë¶ˆ ê¸ˆì•¡",
        # LLMì´ ìì£¼ ìƒì„±í•˜ëŠ” ë³„ì¹­
        "completed_payment_count": "ì™„ë£Œ ê²°ì œ ê±´ìˆ˜",
        "total_payments": "ì´ ê²°ì œ ê±´ìˆ˜",
        "avg_payment": "í‰ê·  ê²°ì œ ê¸ˆì•¡",
        "canceled_count": "ì·¨ì†Œ ê±´ìˆ˜",
        "failed_count": "ì‹¤íŒ¨ ê±´ìˆ˜",
        "total_sales": "ì´ ë§¤ì¶œ",
        "total_transactions": "ì´ ê±°ë˜ ê±´ìˆ˜",
        # ì¼ë°˜ ì»¬ëŸ¼ëª…
        "amount": "ê¸ˆì•¡",
        "merchant_id": "ê°€ë§¹ì  ID",
        "status": "ìƒíƒœ",
        "method": "ê²°ì œìˆ˜ë‹¨",
    }

    # ê¸ˆì•¡ ê´€ë ¨ í‚¤ì›Œë“œ (í†µí™” í¬ë§·íŒ… ì ìš©)
    AMOUNT_KEYWORDS = ["amount", "fee", "total", "sum", "price", "balance", "net"]

    def format_value(key: str, value) -> str:
        """ê°’ì„ í¬ë§·íŒ… (ê¸ˆì•¡ì€ í†µí™” í˜•ì‹, ê±´ìˆ˜ëŠ” "ê±´" ì ‘ë¯¸ì‚¬)"""
        from decimal import Decimal

        if value is None:
            return "-"

        # ìˆ«ì íƒ€ì… ì²´í¬ (int, float, Decimal, ìˆ«ì ë¬¸ìì—´)
        numeric_value = None
        if isinstance(value, (int, float, Decimal)):
            numeric_value = float(value)
        elif isinstance(value, str):
            try:
                numeric_value = float(value)
            except ValueError:
                pass

        if numeric_value is not None:
            int_val = int(numeric_value)
            # ê¸ˆì•¡ ê´€ë ¨ í•„ë“œë©´ í†µí™” í¬ë§·
            if any(kw in key.lower() for kw in AMOUNT_KEYWORDS):
                return f"â‚©{int_val:,}"
            # count í•„ë“œë©´ "ê±´" ì ‘ë¯¸ì‚¬
            elif "count" in key.lower():
                return f"{int_val:,}ê±´"
            else:
                return f"{int_val:,}"

        return _escape_markdown_table_cell(value)

    def get_label(key: str) -> str:
        """ì»¬ëŸ¼ëª…ì„ í•œê¸€ ë¼ë²¨ë¡œ ë³€í™˜"""
        if key in COLUMN_LABELS:
            return COLUMN_LABELS[key]
        # ìŠ¤ë„¤ì´í¬ ì¼€ì´ìŠ¤ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ê³  Title Case ì ìš©
        return key.replace("_", " ").title()

    # Markdown í…Œì´ë¸” ìƒì„±
    lines = [
        "## ğŸ“Š ì§‘ê³„ ê²°ê³¼\n",
        "| í•­ëª© | ê°’ |",
        "|------|------|"
    ]

    for key, value in row.items():
        label = get_label(key)
        formatted = format_value(key, value)
        # escape ì²˜ë¦¬ëœ ë¼ë²¨ê³¼ ê°’ ì‚¬ìš©
        safe_label = _escape_markdown_table_cell(label)
        lines.append(f"| {safe_label} | {formatted} |")

    # êµ¬ë¶„ì„ 
    lines.append("\n---\n")

    # ì¡°íšŒ ì¡°ê±´ (humanized ì‚¬ìš©)
    if aggregation_context:
        humanized_filters = aggregation_context.get("humanizedFilters", [])
        based_on_filters = aggregation_context.get("basedOnFilters", [])

        # humanizedFilters ìš°ì„ , ì—†ìœ¼ë©´ basedOnFilters ì‚¬ìš©
        filters_to_show = humanized_filters if humanized_filters else based_on_filters

        if filters_to_show:
            lines.append("**ğŸ“Œ ì¡°íšŒ ì¡°ê±´**")
            for filter_desc in filters_to_show:
                safe_filter = _escape_markdown_table_cell(filter_desc)
                lines.append(f"- {safe_filter}")
            lines.append("")

        # ê¸°íƒ€ ì •ë³´
        info_items = []
        source_count = aggregation_context.get("sourceRowCount")
        if source_count is not None:
            info_items.append(f"- ëŒ€ìƒ ë°ì´í„°: {source_count:,}ê±´")

        query_type = aggregation_context.get("queryType")
        if query_type:
            qtype_label = "ìƒˆ ì¿¼ë¦¬ ì‹¤í–‰" if query_type == "NEW_QUERY" else "ì¡°ê±´ ì¶”ê°€"
            info_items.append(f"- ì²˜ë¦¬ ë°©ì‹: {qtype_label}")

        if info_items:
            lines.append("**ğŸ“Œ ê¸°íƒ€ ì •ë³´**")
            lines.extend(info_items)

    return "\n".join(lines)


def compose_sql_render_spec(
    result: Dict[str, Any],
    question: str,
    llm_chart_type: Optional[str] = None,
    insight_template: Optional[str] = None
) -> Dict[str, Any]:
    """SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ RenderSpecìœ¼ë¡œ ë³€í™˜

    - ì°¨íŠ¸ ìš”ì²­: ì°¨íŠ¸ RenderSpec ë°˜í™˜ (TC-001)
    - 1000ê±´ ì´ˆê³¼: ë‹¤ìš´ë¡œë“œ RenderSpec (í…Œì´ë¸” í‘œì‹œ ì•ˆí•¨)
    - 1000ê±´ ì´í•˜: ë¯¸ë¦¬ë³´ê¸° 10ê±´ + ì „ì²´ë³´ê¸° ëª¨ë‹¬

    Args:
        result: SQL ì‹¤í–‰ ê²°ê³¼
        question: ì‚¬ìš©ì ì§ˆë¬¸
        llm_chart_type: LLMì´ ì¶”ì²œí•œ ì°¨íŠ¸ íƒ€ì… (ì„ íƒì )
        insight_template: LLMì´ ìƒì„±í•œ ì¸ì‚¬ì´íŠ¸ í…œí”Œë¦¿ (ì„ íƒì )
    """
    # TC-001: ì°¨íŠ¸ ë Œë”ë§ íƒ€ì… ê°ì§€
    # LLMì´ ìœ íš¨í•œ ì°¨íŠ¸ íƒ€ì…ì„ ì¶”ì²œí–ˆìœ¼ë©´ ì°¨íŠ¸ë¡œ ë Œë”ë§
    render_type = _detect_render_type_from_message(question)

    # LLM ì°¨íŠ¸ íƒ€ì…ì´ ìœ íš¨í•˜ë©´(noneì´ ì•„ë‹ˆë©´) ì°¨íŠ¸ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬
    if llm_chart_type and llm_chart_type in ["line", "bar", "pie"]:
        logger.info(f"[compose_sql_render_spec] LLM chart type detected: {llm_chart_type}")
        return _compose_chart_render_spec(result, question, llm_chart_type, insight_template)

    # ë©”ì‹œì§€ì—ì„œ ì°¨íŠ¸ í‚¤ì›Œë“œ ê°ì§€
    if render_type == "chart":
        return _compose_chart_render_spec(result, question, llm_chart_type, insight_template)

    data = result.get("data", [])
    row_count = result.get("rowCount", 0)
    total_count = result.get("totalCount") or row_count
    is_truncated = result.get("isTruncated", False)
    PREVIEW_LIMIT = 10  # ë¯¸ë¦¬ë³´ê¸° í–‰ ìˆ˜
    MAX_DISPLAY_ROWS = 1000  # í™”ë©´ í‘œì‹œ ìµœëŒ€ ê±´ìˆ˜

    # 1000ê±´ ì´ˆê³¼: ë‹¤ìš´ë¡œë“œ RenderSpec ë°˜í™˜
    if is_truncated:
        return {
            "type": "download",
            "title": "ëŒ€ìš©ëŸ‰ ë°ì´í„° ì¡°íšŒ",
            "download": {
                "totalRows": total_count,
                "maxDisplayRows": MAX_DISPLAY_ROWS,
                "message": f"ì¡°íšŒ ê²°ê³¼ê°€ {total_count:,}ê±´ìœ¼ë¡œ í™”ë©´ í‘œì‹œ ì œí•œ({MAX_DISPLAY_ROWS:,}ê±´)ì„ ì´ˆê³¼í•©ë‹ˆë‹¤.",
                "sql": result.get("sql"),
                "formats": ["csv"]
            },
            "metadata": {
                "sql": result.get("sql"),
                "executionTimeMs": result.get("executionTimeMs"),
                "mode": "text_to_sql"
            }
        }

    if not data:
        return {
            "type": "text",
            "title": "ì¡°íšŒ ê²°ê³¼",
            "text": {
                "content": "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "format": "plain"
            },
            "metadata": {
                "sql": result.get("sql"),
                "executionTimeMs": result.get("executionTimeMs")
            }
        }

    # ì§‘ê³„ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
    is_aggregation = result.get("isAggregation", False)
    aggregation_context = result.get("aggregationContext")

    # ë‹¨ì¼ í–‰ + ì§‘ê³„ ê²°ê³¼ì²˜ëŸ¼ ë³´ì´ë©´ Markdown í…Œì´ë¸”ë¡œ í‘œì‹œ
    if row_count == 1 and len(data[0]) <= 5:
        row = data[0]
        # Markdown í…Œì´ë¸” + ì¡°íšŒ ì¡°ê±´ ìƒì„±
        content = _format_aggregation_as_markdown_table(row, aggregation_context)

        return {
            "type": "text",
            "title": "ì§‘ê³„ ê²°ê³¼",
            "text": {
                "content": content,
                "format": "markdown"
            },
            "metadata": {
                "sql": result.get("sql"),
                "executionTimeMs": result.get("executionTimeMs"),
                "mode": "text_to_sql",
                "isAggregation": is_aggregation,
                "aggregationContext": aggregation_context
            }
        }

    # ë‹¤ì¤‘ í–‰: í…Œì´ë¸”ë¡œ í‘œì‹œ (ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ)
    if data:
        columns = list(data[0].keys())
        column_defs = []
        for col in columns:
            col_def = {
                "key": col,  # UI TableRenderer í˜¸í™˜
                "label": col.replace("_", " ").title(),  # TableRenderer expects 'label'
                "field": col,
                "headerName": col.replace("_", " ").title()
            }
            # ê¸ˆì•¡ í•„ë“œ ê°ì§€
            if any(kw in col.lower() for kw in ["amount", "fee", "net", "total", "price"]):
                col_def["type"] = "currency"
                col_def["currencyCode"] = "KRW"
            # ë‚ ì§œ í•„ë“œ ê°ì§€
            elif any(kw in col.lower() for kw in ["date", "time", "at", "created", "updated"]):
                col_def["type"] = "datetime"
            column_defs.append(col_def)

        # ë¯¸ë¦¬ë³´ê¸°ìš© ë°ì´í„° (ìµœëŒ€ PREVIEW_LIMITê±´)
        preview_data = data[:PREVIEW_LIMIT]
        has_more = row_count > PREVIEW_LIMIT

        # íƒ€ì´í‹€: ë¯¸ë¦¬ë³´ê¸°ì¸ ê²½ìš° í‘œì‹œ
        if has_more:
            title = f"ì¡°íšŒ ê²°ê³¼ ({row_count}ê±´ ì¤‘ {PREVIEW_LIMIT}ê±´ ë¯¸ë¦¬ë³´ê¸°)"
        else:
            title = f"ì¡°íšŒ ê²°ê³¼ ({row_count}ê±´)"

        return {
            "type": "table",
            "title": title,
            # TC-004: ìµœìƒìœ„ pagination ì¶”ê°€
            "pagination": {
                "totalRows": row_count,
                "totalPages": math.ceil(row_count / PREVIEW_LIMIT) if row_count > 0 else 1,
                "pageSize": PREVIEW_LIMIT,
                "hasMore": has_more
            },
            "table": {
                "columns": column_defs,
                "data": preview_data,  # ë¯¸ë¦¬ë³´ê¸°ë§Œ ì „ì†¡
                "dataRef": "data.rows",
                "actions": [
                    {"action": "fullscreen", "label": "ì „ì²´ë³´ê¸°"},
                    {"action": "export-csv", "label": "CSV ë‹¤ìš´ë¡œë“œ"}
                ],
                "pagination": {
                    "enabled": False,  # ë¯¸ë¦¬ë³´ê¸°ì—ì„œëŠ” í˜ì´ì§€ë„¤ì´ì…˜ ë¹„í™œì„±í™”
                    "pageSize": PREVIEW_LIMIT,
                    "totalRows": row_count
                }
            },
            # ì „ì²´ ë°ì´í„°ëŠ” ë³„ë„ë¡œ ì €ì¥ (ëª¨ë‹¬ì—ì„œ ì‚¬ìš©)
            "fullData": data if has_more else None,
            "preview": {
                "enabled": has_more,
                "previewRows": PREVIEW_LIMIT,
                "totalRows": row_count,
                "message": f"ì „ì²´ {row_count}ê±´ ì¤‘ {PREVIEW_LIMIT}ê±´ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ì „ì²´ë³´ê¸° ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
            },
            "metadata": {
                "sql": result.get("sql"),
                "executionTimeMs": result.get("executionTimeMs"),
                "mode": "text_to_sql"
            }
        }

    return {
        "type": "text",
        "title": "ê²°ê³¼",
        "text": {
            "content": f"ì¡°íšŒ ì™„ë£Œ: {row_count}ê±´",
            "format": "plain"
        }
    }
