"""
Chat API - Step 6: LangChain + Natural Language Processing
ìžì—°ì–´ â†’ QueryPlan â†’ Core API â†’ RenderSpec
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
import httpx
import logging
import os
import re
import uuid
from datetime import datetime


def to_camel(string: str) -> str:
    """snake_caseë¥¼ camelCaseë¡œ ë³€í™˜"""
    components = string.split('_')
    return components[0] + ''.join(x.title() for x in components[1:])


def summarize_query_plan(query_plan: Dict[str, Any]) -> str:
    """QueryPlanì„ ê°„ë‹¨í•œ ìš”ì•½ ë¬¸ìžì—´ë¡œ ë³€í™˜"""
    parts = []

    entity = query_plan.get("entity", "")
    if entity:
        parts.append(entity)

    # í•„í„° ìš”ì•½
    filters = query_plan.get("filters", [])
    if filters:
        filter_strs = [f"{f.get('field')} {f.get('operator')} {f.get('value')}" for f in filters]
        parts.append(f"filters:[{', '.join(filter_strs)}]")

    # limit
    if query_plan.get("limit"):
        parts.append(f"limit:{query_plan['limit']}")

    # orderBy
    order_by = query_plan.get("orderBy", [])
    if order_by:
        order_strs = [f"{o.get('field')} {o.get('direction', 'asc')}" for o in order_by]
        parts.append(f"orderBy:[{', '.join(order_strs)}]")

    return ", ".join(parts) if parts else "[ì¿¼ë¦¬ ì—†ìŒ]"


def build_conversation_context(history: List["ChatMessageItem"]) -> str:
    """ì´ì „ ëŒ€í™”ë¥¼ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë‹¤ì¤‘ ê²°ê³¼ ìƒí™© ëª…ì‹œ í¬í•¨)"""
    if not history:
        return ""

    context = "## ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸\n\n"

    # ============================================
    # [NEW] ë‹¤ì¤‘ ê²°ê³¼ ìƒí™© ëª…ì‹œ ì„¹ì…˜
    # ============================================
    result_messages = []
    for i, msg in enumerate(history):
        if msg.role == "assistant" and msg.queryResult:
            entity = msg.queryPlan.get("entity", "unknown") if msg.queryPlan else "unknown"
            count = msg.queryResult.get("totalCount", 0)
            # í•„í„° ì •ë³´ë„ ì¶”ê°€ (ê°™ì€ entityë¼ë„ ì¡°ê±´ì´ ë‹¤ë¥¼ ìˆ˜ ìžˆìŒ)
            filters = msg.queryPlan.get("filters", []) if msg.queryPlan else []
            filter_desc = ""
            if filters:
                filter_strs = [f"{f.get('field')}={f.get('value')}" for f in filters[:2]]
                filter_desc = f" ({', '.join(filter_strs)})"
            result_messages.append({
                "index": i,
                "entity": entity,
                "count": count,
                "filter_desc": filter_desc,
                "is_latest": False
            })

    if result_messages:
        result_messages[-1]["is_latest"] = True  # ë§ˆì§€ë§‰ì´ ì§ì „ ê²°ê³¼

        context += "### ðŸ“Š í˜„ìž¬ ì„¸ì…˜ì˜ ì¡°íšŒ ê²°ê³¼ í˜„í™©\n"
        for r in result_messages:
            marker = "ðŸ‘‰ (ì§ì „)" if r["is_latest"] else ""
            context += f"- ê²°ê³¼ #{r['index']}: {r['entity']} {r['count']}ê±´{r['filter_desc']} {marker}\n"

        if len(result_messages) > 1:
            # ë‹¤ì¤‘ ê²°ê³¼ ê²½ê³  - LLMì´ ì£¼ëª©í•˜ë„ë¡
            entities = set(r["entity"] for r in result_messages)
            if len(entities) > 1:
                context += f"\nâš ï¸ **ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê²°ê³¼ê°€ {len(result_messages)}ê°œ ìžˆìŠµë‹ˆë‹¤** ({', '.join(entities)})\n"
                context += "â†’ ì‚¬ìš©ìžê°€ íŠ¹ì • ê²°ê³¼ë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ needs_result_clarification=true ê¶Œìž¥\n"
            else:
                context += f"\nðŸ“Œ ë™ì¼ ì—”í‹°í‹°({list(entities)[0]}) ê²°ê³¼ê°€ {len(result_messages)}ê°œ ìžˆìŠµë‹ˆë‹¤ (ì¡°ê±´ì´ ë‹¤ë¦„).\n"
                context += "â†’ ì°¸ì¡° í‘œí˜„ ì—†ì´ ì§‘ê³„/í•„í„° ìš”ì²­ ì‹œ needs_result_clarification=true ê³ ë ¤\n"
        context += "\n"

    # ============================================
    # ëŒ€í™” ížˆìŠ¤í† ë¦¬ (ê¸°ì¡´ ìœ ì§€)
    # ============================================
    context += "### ëŒ€í™” ížˆìŠ¤í† ë¦¬\n"
    for msg in history[-5:]:
        if msg.role == 'user':
            context += f"ì‚¬ìš©ìž: {msg.content}\n"
        else:
            # queryPlan ìš”ì•½ í¬í•¨
            if msg.queryPlan:
                plan_summary = summarize_query_plan(msg.queryPlan)
                context += f"ì–´ì‹œìŠ¤í„´íŠ¸: [ì¿¼ë¦¬: {plan_summary}]\n"
            else:
                context += f"ì–´ì‹œìŠ¤í„´íŠ¸: [ê²°ê³¼ í‘œì‹œë¨]\n"

            # ì§‘ê³„ ê²°ê³¼ê°’ í¬í•¨ (ì¤‘ìš”: í›„ì† ê³„ì‚°ìš©)
            if msg.renderSpec and msg.renderSpec.get("type") == "text":
                text_content = msg.renderSpec.get("text", {}).get("content", "")
                if text_content and ("í•©ê³„" in text_content or "$" in text_content or "ì›" in text_content):
                    context += f"  â†’ ì§‘ê³„ ê²°ê³¼: {text_content}\n"

    # ============================================
    # í›„ì† ì§ˆë¬¸ ì²˜ë¦¬ ê·œì¹™ (ê°œì„ )
    # ============================================
    context += "\n### í›„ì† ì§ˆë¬¸ ì²˜ë¦¬ ê·œì¹™\n"
    context += "1. 'ì´ì¤‘ì—', 'ì—¬ê¸°ì„œ', 'ì§ì „', 'ë°©ê¸ˆ' ë“± ì°¸ì¡° í‘œí˜„ â†’ **ì§ì „ ê²°ê³¼ ì‚¬ìš©**, needs_result_clarification=false\n"
    context += "2. ì°¸ì¡° í‘œí˜„ ì—†ì´ ì§‘ê³„/í•„í„° ìš”ì²­ + ë‹¤ì¤‘ ê²°ê³¼ â†’ needs_result_clarification=true ê³ ë ¤\n"
    context += "3. í›„ì† ì§ˆë¬¸ì—ì„œëŠ” **ì´ì „ ì—”í‹°í‹° ìœ ì§€** (ë‹¤ë¥¸ ì—”í‹°í‹°ë¡œ ë³€ê²½ ê¸ˆì§€)\n"
    context += "4. ì´ì „ ì§‘ê³„ ê²°ê³¼ì— ëŒ€í•œ ì‚°ìˆ  ì—°ì‚° â†’ query_intent=direct_answer\n"
    return context


def get_previous_query_plan(history: List["ChatMessageItem"]) -> Optional[Dict[str, Any]]:
    """ì´ì „ ëŒ€í™”ì—ì„œ ë§ˆì§€ë§‰ queryPlan ì¶”ì¶œ"""
    if not history:
        return None

    # ì—­ìˆœìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ê°€ìž¥ ìµœê·¼ assistantì˜ queryPlan ì°¾ê¸°
    for msg in reversed(history):
        if msg.role == 'assistant' and msg.queryPlan:
            return msg.queryPlan
    return None


def extract_previous_results(history: List["ChatMessageItem"]) -> List[Dict[str, Any]]:
    """ì´ì „ ëŒ€í™”ì—ì„œ ì¡°íšŒ/ì§‘ê³„ ê²°ê³¼ ìš”ì•½ ì¶”ì¶œ (Intent Classificationìš©)

    ì‹¤ì œ ë°ì´í„° ê°’ë„ ì¶”ì¶œí•˜ì—¬ LLMì´ ê³„ì‚°í•  ìˆ˜ ìžˆë„ë¡ í•¨
    """
    results = []
    if not history:
        return results

    for i, msg in enumerate(history):
        if msg.role == "assistant":
            result_info = {
                "index": i,
                "entity": None,
                "count": 0,
                "aggregation": None,
                "data_summary": None,  # ì‹¤ì œ ë°ì´í„° ìš”ì•½
                "total_amount": None   # ê¸ˆì•¡ í•©ê³„ (ìžˆëŠ” ê²½ìš°)
            }

            # QueryResultê°€ ìžˆìœ¼ë©´ ì¡°íšŒ ê²°ê³¼
            if msg.queryResult:
                logger.info(f"[extract_previous_results] msg #{i} has queryResult with keys: {list(msg.queryResult.keys())}")
                result_info["count"] = msg.queryResult.get("totalCount", 0)
                if msg.queryPlan:
                    result_info["entity"] = msg.queryPlan.get("entity", "unknown")

                # ì‹¤ì œ ë°ì´í„°ì—ì„œ ê¸ˆì•¡ í•©ê³„ ì¶”ì¶œ
                data_obj = msg.queryResult.get("data", {})
                # data is an object with 'rows' property according to query-result.schema.json
                rows = data_obj.get("rows", []) if isinstance(data_obj, dict) else []
                logger.info(f"[extract_previous_results] msg #{i} rows length: {len(rows) if rows else 0}")

                if rows:
                    # amount í•„ë“œê°€ ìžˆìœ¼ë©´ í•©ê³„ ê³„ì‚°
                    amounts = []
                    for row_idx, row in enumerate(rows):
                        if isinstance(row, dict):
                            logger.info(f"[extract_previous_results] msg #{i} row #{row_idx} keys: {list(row.keys())}")
                            # amount, totalAmount, ê¸ˆì•¡ ë“± ë‹¤ì–‘í•œ í•„ë“œëª… ì²´í¬
                            for field in ["amount", "totalAmount", "total_amount", "price", "ê¸ˆì•¡"]:
                                if field in row and row[field] is not None:
                                    try:
                                        amounts.append(float(row[field]))
                                        logger.info(f"[extract_previous_results] msg #{i} row #{row_idx} found {field}={row[field]}")
                                    except (ValueError, TypeError) as e:
                                        logger.info(f"[extract_previous_results] msg #{i} row #{row_idx} error converting {field}: {e}")
                                        pass
                                    break
                        else:
                            logger.info(f"[extract_previous_results] msg #{i} row #{row_idx} is not a dict: {type(row)}")

                    if amounts:
                        result_info["total_amount"] = sum(amounts)
                        result_info["data_summary"] = f"ê¸ˆì•¡ í•©ê³„: ${result_info['total_amount']:,.0f} ({len(amounts)}ê±´)"
                        logger.info(f"[extract_previous_results] msg #{i} extracted total_amount: ${result_info['total_amount']:,.0f} from {len(amounts)} amounts")
                    else:
                        logger.info(f"[extract_previous_results] msg #{i} no amounts found in {len(data)} rows")

            # RenderSpecì´ text íƒ€ìž…ì´ë©´ ì§‘ê³„ ê²°ê³¼ì¼ ìˆ˜ ìžˆìŒ
            if msg.renderSpec and msg.renderSpec.get("type") == "text":
                text_content = msg.renderSpec.get("text", {}).get("content", "")
                if text_content:
                    result_info["aggregation"] = text_content

                    # í…ìŠ¤íŠ¸ì—ì„œ ê¸ˆì•¡ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: ê´„í˜¸ ì•ˆ ì „ì²´ ê¸ˆì•¡ > ì¶•ì•½ ê¸ˆì•¡)
                    if result_info["total_amount"] is None:
                        # 1ìˆœìœ„: ê´„í˜¸ ì•ˆì˜ ì „ì²´ ê¸ˆì•¡ (ì˜ˆ: "$2.88M ($2,878,000)" â†’ 2878000)
                        full_amount_match = re.search(r'\(\$?([\d,]+)\)', text_content)
                        if full_amount_match:
                            try:
                                result_info["total_amount"] = float(full_amount_match.group(1).replace(',', ''))
                                logger.info(f"[extract_previous_results] Extracted full amount from parens: ${result_info['total_amount']:,.0f}")
                            except ValueError:
                                pass

                        # 2ìˆœìœ„: M/K ì ‘ë¯¸ì‚¬ ì²˜ë¦¬ (ì˜ˆ: "$2.88M" â†’ 2880000)
                        if result_info["total_amount"] is None:
                            abbrev_match = re.search(r'\$?([\d,]+(?:\.\d+)?)\s*([MK])', text_content)
                            if abbrev_match:
                                try:
                                    value = float(abbrev_match.group(1).replace(',', ''))
                                    suffix = abbrev_match.group(2)
                                    if suffix == 'M':
                                        value *= 1_000_000
                                    elif suffix == 'K':
                                        value *= 1_000
                                    result_info["total_amount"] = value
                                    logger.info(f"[extract_previous_results] Extracted abbreviated amount: ${result_info['total_amount']:,.0f}")
                                except ValueError:
                                    pass

                        # 3ìˆœìœ„: ì¼ë°˜ ê¸ˆì•¡ (ì˜ˆ: "$1,234,567")
                        if result_info["total_amount"] is None:
                            simple_match = re.search(r'\$?([\d,]+(?:\.\d+)?)', text_content)
                            if simple_match:
                                try:
                                    result_info["total_amount"] = float(simple_match.group(1).replace(',', ''))
                                    logger.info(f"[extract_previous_results] Extracted simple amount: ${result_info['total_amount']:,.0f}")
                                except ValueError:
                                    pass

            # ì¡°íšŒ ê²°ê³¼ë‚˜ ì§‘ê³„ ê²°ê³¼ê°€ ìžˆìœ¼ë©´ ì¶”ê°€
            if result_info["count"] > 0 or result_info["aggregation"]:
                results.append(result_info)
                logger.info(f"[extract_previous_results] Added result #{len(results)}: entity={result_info['entity']}, count={result_info['count']}, total_amount={result_info['total_amount']}")

    logger.info(f"[extract_previous_results] Total results extracted: {len(results)}")
    return results


def merge_filters(previous_plan: Dict[str, Any], new_plan: Dict[str, Any]) -> Dict[str, Any]:
    """ì´ì „ í•„í„°ì™€ ìƒˆ í•„í„°ë¥¼ ë³‘í•©"""
    if not previous_plan:
        return new_plan

    # clarification ìš”ì²­ì´ë©´ ë³‘í•©í•˜ì§€ ì•ŠìŒ
    if new_plan.get("needs_clarification"):
        return new_plan

    # ì´ì „ í•„í„° ê°€ì ¸ì˜¤ê¸°
    prev_filters = previous_plan.get("filters", [])
    new_filters = new_plan.get("filters", [])

    # ìƒˆ í•„í„°ì˜ í•„ë“œëª… ëª©ë¡
    new_filter_fields = {f.get("field") for f in new_filters}

    # ì´ì „ í•„í„° ì¤‘ ìƒˆ í•„í„°ì— ì—†ëŠ” ê²ƒë§Œ ë³‘í•© (ì¤‘ë³µ í•„ë“œ ë°©ì§€)
    merged_filters = list(new_filters)  # ìƒˆ í•„í„° ìš°ì„ 
    for prev_filter in prev_filters:
        if prev_filter.get("field") not in new_filter_fields:
            merged_filters.append(prev_filter)

    # ë³‘í•©ëœ ê²°ê³¼
    merged_plan = dict(new_plan)
    if merged_filters:
        merged_plan["filters"] = merged_filters

    # ì´ì „ entity ìœ ì§€ (ìƒˆ planì— entityê°€ ì—†ìœ¼ë©´)
    if not merged_plan.get("entity") and previous_plan.get("entity"):
        merged_plan["entity"] = previous_plan["entity"]

    # ì´ì „ limit ìœ ì§€ (ìƒˆ planì´ ê¸°ë³¸ê°’ 10ì´ë©´)
    if merged_plan.get("limit") == 10 and previous_plan.get("limit"):
        merged_plan["limit"] = previous_plan["limit"]

    return merged_plan

from app.services.query_planner import get_query_planner, IntentType
from app.services.render_composer import get_render_composer
from app.services.rag_service import get_rag_service

logger = logging.getLogger(__name__)

router = APIRouter()

# Configuration
CORE_API_URL = os.getenv("CORE_API_URL", "http://localhost:8080")
ENABLE_QUERY_PLAN_VALIDATION = os.getenv("ENABLE_QUERY_PLAN_VALIDATION", "true").lower() == "true"


class ChatMessageItem(BaseModel):
    """ëŒ€í™” ë©”ì‹œì§€ ì•„ì´í…œ"""
    id: str
    role: str  # 'user' | 'assistant'
    content: str
    timestamp: str
    status: Optional[str] = None
    renderSpec: Optional[Dict[str, Any]] = None
    queryResult: Optional[Dict[str, Any]] = None
    queryPlan: Optional[Dict[str, Any]] = None  # ì´ì „ ì¿¼ë¦¬ ì¡°ê±´ ì €ìž¥ìš©


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­"""
    message: str
    conversation_id: Optional[str] = None
    session_id: Optional[str] = Field(default=None, alias="sessionId")
    conversation_history: Optional[List[ChatMessageItem]] = Field(default=None, alias="conversationHistory")

    class Config:
        populate_by_name = True


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ - UI íƒ€ìž…ê³¼ ì¼ì¹˜"""
    request_id: str = Field(alias="requestId")
    render_spec: Dict[str, Any] = Field(alias="renderSpec")
    query_result: Optional[Dict[str, Any]] = Field(default=None, alias="queryResult")  # filter_local ì‹œ None ê°€ëŠ¥
    query_plan: Dict[str, Any] = Field(alias="queryPlan")  # ì´ë²ˆ ì¿¼ë¦¬ ì¡°ê±´ (í›„ì† ì§ˆë¬¸ìš©)
    ai_message: Optional[str] = Field(default=None, alias="aiMessage")
    timestamp: str

    class Config:
        populate_by_name = True


@router.post("/chat", response_model=ChatResponse, response_model_by_alias=True)
async def chat(request: ChatRequest):
    """
    Step 6: LangChain ê¸°ë°˜ ìžì—°ì–´ ì²˜ë¦¬

    Flow:
    1. ì‚¬ìš©ìž ë©”ì‹œì§€ ìˆ˜ì‹ 
    2. QueryPlannerServiceë¡œ ìžì—°ì–´ â†’ QueryPlan ë³€í™˜
    3. Core API í˜¸ì¶œ
    4. RenderComposerServiceë¡œ QueryResult â†’ RenderSpec ë³€í™˜
    5. RenderSpec ë°˜í™˜
    """
    start_time = datetime.utcnow()
    conversation_id = request.conversation_id or str(uuid.uuid4())
    request_id = f"req-{uuid.uuid4().hex[:8]}"

    logger.info(f"[{request_id}] Received message: {request.message}")

    processing_info = {
        "requestId": request_id,
        "stages": []
    }

    try:
        # Stage 0: Intent Classification (2ë‹¨ê³„ ë¶„ë¥˜)
        stage_start = datetime.utcnow()
        query_planner = get_query_planner()

        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        conversation_context = None
        previous_results = []
        if request.conversation_history:
            conversation_context = build_conversation_context(request.conversation_history)
            previous_results = extract_previous_results(request.conversation_history)
            logger.info(f"[{request_id}] Using conversation context with {len(request.conversation_history)} messages")
            logger.info(f"[{request_id}] Found {len(previous_results)} previous results for intent classification")

        # 1ë‹¨ê³„: Intent ë¶„ë¥˜ (ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë¹ ë¥´ê²Œ)
        intent_result = await query_planner.classify_intent(
            request.message,
            conversation_context or "",
            previous_results
        )
        logger.info(f"[{request_id}] Intent classification: {intent_result.intent.value}, confidence={intent_result.confidence:.2f}")

        # direct_answerë©´ ë°”ë¡œ ì‘ë‹µ ë°˜í™˜ (QueryPlan ìƒì„± ìŠ¤í‚µ)
        if intent_result.intent == IntentType.DIRECT_ANSWER and intent_result.direct_answer_text:
            logger.info(f"[{request_id}] Direct answer detected, skipping QueryPlan generation")
            logger.info(f"[{request_id}] Direct answer text: {intent_result.direct_answer_text}")

            return ChatResponse(
                request_id=request_id,
                query_plan={
                    "query_intent": "direct_answer",
                    "requestId": request_id
                },
                query_result=None,
                render_spec={
                    "type": "text",
                    "text": {
                        "content": intent_result.direct_answer_text,
                        "format": "markdown"
                    },
                    "metadata": {
                        "intent": "direct_answer",
                        "confidence": intent_result.confidence,
                        "reasoning": intent_result.reasoning
                    }
                },
                timestamp=datetime.utcnow().isoformat() + "Z"
            )

        processing_info["stages"].append({
            "name": "Intent Classification",
            "duration": (datetime.utcnow() - stage_start).total_seconds() * 1000,
            "result": intent_result.intent.value
        })

        # Stage 1: Natural Language â†’ QueryPlan
        stage_start = datetime.utcnow()

        query_plan = await query_planner.generate_query_plan(
            request.message,
            conversation_context=conversation_context,
            enable_validation=ENABLE_QUERY_PLAN_VALIDATION
        )

        # LLMì´ íŒë‹¨í•œ ì˜ë„ì— ë”°ë¼ í•„í„° ë³‘í•© ê²°ì •
        query_intent = query_plan.get("query_intent", "new_query")
        logger.info(f"[{request_id}] Query intent: {query_intent}")

        if query_intent == "refine_previous":
            if request.conversation_history:
                previous_plan = get_previous_query_plan(request.conversation_history)
                if previous_plan:
                    logger.info(f"[{request_id}] Intent: refine_previous, merging with previous filters")
                    logger.info(f"[{request_id}] Previous plan: {previous_plan}")
                    query_plan = merge_filters(previous_plan, query_plan)
                    logger.info(f"[{request_id}] Merged plan: {query_plan}")
        elif query_intent == "filter_local":
            # í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ í•„í„°ë§: Core API í˜¸ì¶œ ì—†ì´ í•„í„° ì¡°ê±´ë§Œ ë°˜í™˜
            logger.info(f"[{request_id}] Intent: filter_local, client-side filtering")

            # entityê°€ ì—†ìœ¼ë©´ ì´ì „ queryPlanì—ì„œ ìƒì†
            if not query_plan.get("entity") and request.conversation_history:
                previous_plan = get_previous_query_plan(request.conversation_history)
                if previous_plan and previous_plan.get("entity"):
                    query_plan["entity"] = previous_plan["entity"]
                    logger.info(f"[{request_id}] Inherited entity from previous plan: {query_plan['entity']}")

            # ì´ì „ ê²°ê³¼ê°€ ìžˆëŠ” ë©”ì‹œì§€ë“¤ ì°¾ê¸°
            result_messages = []
            if request.conversation_history:
                logger.info(f"[{request_id}] Checking {len(request.conversation_history)} messages in history")
                for i, msg in enumerate(request.conversation_history):
                    has_query_result = msg.queryResult is not None
                    logger.info(f"[{request_id}] Message {i}: role={msg.role}, hasQueryResult={has_query_result}")
                    if msg.role == "assistant" and msg.queryResult:
                        result_messages.append((i, msg))

            logger.info(f"[{request_id}] Found {len(result_messages)} result messages")

            # 1ë‹¨ê³„: LLMì´ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨í–ˆëŠ”ì§€ í™•ì¸
            needs_result_clarification = query_plan.get("needs_result_clarification", False)
            logger.info(f"[{request_id}] 1st stage LLM decision: needs_result_clarification={needs_result_clarification}")

            # 2ë‹¨ê³„: ë‹¤ì¤‘ ê²°ê³¼ + 1ë‹¨ê³„ê°€ Falseë©´ ìƒìœ„ ëª¨ë¸ë¡œ ìž¬íŒë‹¨
            if len(result_messages) > 1 and not needs_result_clarification:
                logger.info(f"[{request_id}] Multiple results but 1st stage said no clarification, invoking 2nd stage check...")

                # ê²°ê³¼ ìš”ì•½ ìƒì„±
                result_summaries = []
                for msg_idx, msg in result_messages:
                    entity = msg.queryPlan.get("entity", "unknown") if msg.queryPlan else "unknown"
                    count = 0
                    if msg.queryResult:
                        if isinstance(msg.queryResult, dict):
                            count = msg.queryResult.get("totalCount", msg.queryResult.get("metadata", {}).get("rowsReturned", 0))
                    filters_str = ""
                    if msg.queryPlan and msg.queryPlan.get("filters"):
                        filters_str = ", ".join([f"{f.get('field')}={f.get('value')}" for f in msg.queryPlan.get("filters", [])[:2]])
                    result_summaries.append({"entity": entity, "count": count, "filters": filters_str})

                # 2ë‹¨ê³„ LLM íŒë‹¨ í˜¸ì¶œ
                needs_result_clarification = await query_planner.check_clarification_needed(
                    user_message=request.message,
                    result_summaries=result_summaries,
                    query_intent=query_intent
                )
                logger.info(f"[{request_id}] 2nd stage LLM decision: needs_result_clarification={needs_result_clarification}")

            if len(result_messages) > 1 and needs_result_clarification:
                # ë‹¤ì¤‘ ê²°ê³¼ + LLMì´ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨: clarification ìš”ì²­
                recent_results = result_messages[-5:]  # ìµœê·¼ 5ê°œë§Œ
                options = []
                indices = []

                for idx, (msg_idx, msg) in enumerate(reversed(recent_results)):
                    # ê²°ê³¼ ìš”ì•½ ë¼ë²¨ ìƒì„±
                    entity = msg.queryPlan.get("entity", "ë°ì´í„°") if msg.queryPlan else "ë°ì´í„°"
                    count = "?"
                    if msg.queryResult:
                        if isinstance(msg.queryResult, dict):
                            count = msg.queryResult.get("totalCount", msg.queryResult.get("metadata", {}).get("rowsReturned", "?"))
                        elif hasattr(msg.queryResult, "totalCount"):
                            count = msg.queryResult.totalCount
                    time_str = msg.timestamp[-8:-3] if msg.timestamp and len(msg.timestamp) >= 8 else ""

                    label = f"ì§ì „: {entity} {count}ê±´ ({time_str})" if idx == 0 else f"{entity} {count}ê±´ ({time_str})"
                    options.append(label)
                    indices.append(msg_idx)

                logger.info(f"[{request_id}] Multiple results found, requesting clarification")

                clarification_render_spec = {
                    "type": "clarification",
                    "clarification": {
                        "question": "ì–´ë–¤ ì¡°íšŒ ê²°ê³¼ë¥¼ í•„í„°ë§í• ê¹Œìš”?",
                        "options": options
                    },
                    "metadata": {
                        "requestId": request_id,
                        "targetResultIndices": indices,
                        "pendingFilters": query_plan.get("filters", []),
                        "generatedAt": datetime.utcnow().isoformat() + "Z"
                    }
                }

                return ChatResponse(
                    request_id=request_id,
                    render_spec=clarification_render_spec,
                    query_result=None,
                    query_plan={**query_plan, "needs_clarification": True, "requestId": request_id},
                    ai_message="ì–´ë–¤ ì¡°íšŒ ê²°ê³¼ë¥¼ í•„í„°ë§í• ê¹Œìš”?",
                    timestamp=datetime.utcnow().isoformat() + "Z"
                )

            # ê²°ê³¼ê°€ 1ê°œ ì´í•˜: í´ë¼ì´ì–¸íŠ¸ì—ì„œ í•„í„°ë§í•˜ë„ë¡ ì‘ë‹µ
            target_index = result_messages[-1][0] if result_messages else -1
            logger.info(f"[{request_id}] Single result, returning filter_local response (target: {target_index})")

            filter_local_render_spec = {
                "type": "filter_local",
                "filter": query_plan.get("filters", []),
                "targetResultIndex": target_index,
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z"
                }
            }

            return ChatResponse(
                request_id=request_id,
                render_spec=filter_local_render_spec,
                query_result=None,
                query_plan={**query_plan, "requestId": request_id},
                ai_message="ì´ì „ ê²°ê³¼ì—ì„œ í•„í„°ë§í•©ë‹ˆë‹¤.",
                timestamp=datetime.utcnow().isoformat() + "Z"
            )
        elif query_intent == "aggregate_local":
            # í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ì§‘ê³„: ì´ì „ ê²°ê³¼ì—ì„œ ì§‘ê³„
            logger.info(f"[{request_id}] Intent: aggregate_local, client-side aggregation")

            # entityê°€ ì—†ìœ¼ë©´ ì´ì „ queryPlanì—ì„œ ìƒì†
            if not query_plan.get("entity") and request.conversation_history:
                previous_plan = get_previous_query_plan(request.conversation_history)
                if previous_plan and previous_plan.get("entity"):
                    query_plan["entity"] = previous_plan["entity"]
                    logger.info(f"[{request_id}] Inherited entity from previous plan: {query_plan['entity']}")

            # ì´ì „ ê²°ê³¼ê°€ ìžˆëŠ” ë©”ì‹œì§€ë“¤ ì°¾ê¸°
            result_messages = []
            if request.conversation_history:
                logger.info(f"[{request_id}] Checking {len(request.conversation_history)} messages in history")
                for i, msg in enumerate(request.conversation_history):
                    has_query_result = msg.queryResult is not None
                    if msg.role == "assistant" and msg.queryResult:
                        result_messages.append((i, msg))

            logger.info(f"[{request_id}] Found {len(result_messages)} result messages for aggregation")

            # ì§‘ê³„ ì •ë³´ ì¶”ì¶œ
            aggregations = query_plan.get("aggregations", [])
            if not aggregations:
                # ê¸°ë³¸ ì§‘ê³„: sum(amount)
                aggregations = [{"function": "sum", "field": "amount", "alias": "totalAmount", "displayLabel": "ê²°ì œ ê¸ˆì•¡ í•©ê³„", "currency": "USD"}]

            # 1ë‹¨ê³„: LLMì´ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨í–ˆëŠ”ì§€ í™•ì¸
            needs_result_clarification = query_plan.get("needs_result_clarification", False)
            logger.info(f"[{request_id}] 1st stage LLM decision (aggregate): needs_result_clarification={needs_result_clarification}")

            # 2ë‹¨ê³„: ë‹¤ì¤‘ ê²°ê³¼ + 1ë‹¨ê³„ê°€ Falseë©´ ìƒìœ„ ëª¨ë¸ë¡œ ìž¬íŒë‹¨
            if len(result_messages) > 1 and not needs_result_clarification:
                logger.info(f"[{request_id}] Multiple results but 1st stage said no clarification, invoking 2nd stage check...")

                # ê²°ê³¼ ìš”ì•½ ìƒì„±
                result_summaries = []
                for msg_idx, msg in result_messages:
                    entity = msg.queryPlan.get("entity", "unknown") if msg.queryPlan else "unknown"
                    count = 0
                    if msg.queryResult:
                        if isinstance(msg.queryResult, dict):
                            count = msg.queryResult.get("totalCount", msg.queryResult.get("metadata", {}).get("rowsReturned", 0))
                    filters_str = ""
                    if msg.queryPlan and msg.queryPlan.get("filters"):
                        filters_str = ", ".join([f"{f.get('field')}={f.get('value')}" for f in msg.queryPlan.get("filters", [])[:2]])
                    result_summaries.append({"entity": entity, "count": count, "filters": filters_str})

                # 2ë‹¨ê³„ LLM íŒë‹¨ í˜¸ì¶œ
                needs_result_clarification = await query_planner.check_clarification_needed(
                    user_message=request.message,
                    result_summaries=result_summaries,
                    query_intent=query_intent
                )
                logger.info(f"[{request_id}] 2nd stage LLM decision (aggregate): needs_result_clarification={needs_result_clarification}")

            if len(result_messages) > 1 and needs_result_clarification:
                # ë‹¤ì¤‘ ê²°ê³¼ + LLMì´ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨: clarification ìš”ì²­
                recent_results = result_messages[-5:]  # ìµœê·¼ 5ê°œë§Œ
                options = []
                indices = []

                for idx, (msg_idx, msg) in enumerate(reversed(recent_results)):
                    entity = msg.queryPlan.get("entity", "ë°ì´í„°") if msg.queryPlan else "ë°ì´í„°"
                    count = "?"
                    if msg.queryResult:
                        if isinstance(msg.queryResult, dict):
                            count = msg.queryResult.get("totalCount", msg.queryResult.get("metadata", {}).get("rowsReturned", "?"))
                    time_str = msg.timestamp[-8:-3] if msg.timestamp and len(msg.timestamp) >= 8 else ""

                    label = f"ì§ì „: {entity} {count}ê±´ ({time_str})" if idx == 0 else f"{entity} {count}ê±´ ({time_str})"
                    options.append(label)
                    indices.append(msg_idx)

                logger.info(f"[{request_id}] Multiple results found, requesting clarification for aggregation")

                clarification_render_spec = {
                    "type": "clarification",
                    "clarification": {
                        "question": "ì–´ë–¤ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í• ê¹Œìš”?",
                        "options": options
                    },
                    "metadata": {
                        "requestId": request_id,
                        "targetResultIndices": indices,
                        "pendingAggregations": aggregations,
                        "aggregationType": "aggregate_local",
                        "generatedAt": datetime.utcnow().isoformat() + "Z"
                    }
                }

                return ChatResponse(
                    request_id=request_id,
                    render_spec=clarification_render_spec,
                    query_result=None,
                    query_plan={**query_plan, "needs_clarification": True, "requestId": request_id},
                    ai_message="ì–´ë–¤ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í• ê¹Œìš”?",
                    timestamp=datetime.utcnow().isoformat() + "Z"
                )

            # ê²°ê³¼ê°€ 1ê°œ ì´í•˜: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì§‘ê³„í•˜ë„ë¡ ì‘ë‹µ
            target_index = result_messages[-1][0] if result_messages else -1
            logger.info(f"[{request_id}] Single result, returning aggregate_local response (target: {target_index})")

            aggregate_local_render_spec = {
                "type": "aggregate_local",
                "aggregations": aggregations,
                "targetResultIndex": target_index,
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z"
                }
            }

            return ChatResponse(
                request_id=request_id,
                render_spec=aggregate_local_render_spec,
                query_result=None,
                query_plan={**query_plan, "requestId": request_id},
                ai_message="ì´ì „ ê²°ê³¼ì—ì„œ ì§‘ê³„í•©ë‹ˆë‹¤.",
                timestamp=datetime.utcnow().isoformat() + "Z"
            )
        elif query_intent == "direct_answer":
            # LLMì´ ì§ì ‘ ë‹µë³€: DB ì¡°íšŒ ì—†ì´ í…ìŠ¤íŠ¸ ì‘ë‹µ
            direct_answer = query_plan.get("direct_answer", "")
            logger.info(f"[{request_id}] Intent: direct_answer, returning LLM response")

            if not direct_answer:
                direct_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            direct_answer_render_spec = {
                "type": "text",
                "title": "ë¶„ì„ ê²°ê³¼",
                "text": {
                    "content": direct_answer,
                    "format": "markdown"
                },
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z"
                }
            }

            return ChatResponse(
                request_id=request_id,
                render_spec=direct_answer_render_spec,
                query_result=None,
                query_plan={**query_plan, "requestId": request_id},
                ai_message=direct_answer,
                timestamp=datetime.utcnow().isoformat() + "Z"
            )
        else:
            logger.info(f"[{request_id}] Intent: new_query, no filter merge")

        query_plan["requestId"] = request_id

        processing_info["stages"].append({
            "name": "query_plan_generation",
            "durationMs": int((datetime.utcnow() - stage_start).total_seconds() * 1000),
            "status": "success"
        })

        logger.info(f"[{request_id}] Final QueryPlan: {query_plan}")

        # Clarification í•„ìš” ì‹œ ì¿¼ë¦¬ ì‹¤í–‰ ì—†ì´ ëŒ€í™”í˜• ì§ˆë¬¸ ë°˜í™˜
        if query_plan.get("needs_clarification"):
            question = query_plan.get("clarification_question", "ì–´ë–¤ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
            logger.info(f"[{request_id}] Clarification needed: {question}")

            # ëŒ€í™”í˜• í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µ (ë²„íŠ¼ ì—†ì´)
            clarification_render_spec = {
                "type": "text",
                "title": "ì¶”ê°€ ì •ë³´ í•„ìš”",
                "text": {
                    "content": question,
                    "format": "plain"
                },
                "metadata": {
                    "requestId": request_id,
                    "generatedAt": datetime.utcnow().isoformat() + "Z"
                }
            }

            clarification_query_result = {
                "requestId": request_id,
                "status": "pending",
                "data": {"rows": [], "aggregations": {}},
                "metadata": {
                    "executionTimeMs": int((datetime.utcnow() - start_time).total_seconds() * 1000),
                    "rowsReturned": 0,
                    "dataSource": "clarification_required"
                }
            }

            return ChatResponse(
                request_id=request_id,
                render_spec=clarification_render_spec,
                query_result=clarification_query_result,
                query_plan=query_plan,
                ai_message=question,
                timestamp=datetime.utcnow().isoformat() + "Z"
            )

        # Stage 2: Call Core API
        stage_start = datetime.utcnow()
        query_result = await call_core_api(query_plan)

        processing_info["stages"].append({
            "name": "core_api_call",
            "durationMs": int((datetime.utcnow() - stage_start).total_seconds() * 1000),
            "status": "success" if query_result.get("status") == "success" else "error"
        })

        logger.info(f"[{request_id}] Core API response status: {query_result.get('status')}")

        # Stage 3: QueryResult â†’ RenderSpec
        stage_start = datetime.utcnow()
        render_composer = get_render_composer()
        render_spec = render_composer.compose(query_result, query_plan, request.message)

        processing_info["stages"].append({
            "name": "render_spec_composition",
            "durationMs": int((datetime.utcnow() - stage_start).total_seconds() * 1000),
            "status": "success"
        })

        # Calculate total processing time
        total_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        processing_info["totalDurationMs"] = total_time

        logger.info(f"[{request_id}] Completed in {total_time}ms")

        return ChatResponse(
            request_id=request_id,
            render_spec=render_spec,
            query_result=query_result,
            query_plan=query_plan,  # í›„ì† ì§ˆë¬¸ì—ì„œ ì´ì „ ì¿¼ë¦¬ ì¡°ê±´ ì°¸ì¡°ìš©
            ai_message=f"'{request.message}'ì— ëŒ€í•œ ê²°ê³¼ìž…ë‹ˆë‹¤.",
            timestamp=datetime.utcnow().isoformat() + "Z"
        )

    except Exception as e:
        logger.error(f"[{request_id}] Error: {e}", exc_info=True)

        total_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        processing_info["totalDurationMs"] = total_time
        processing_info["error"] = str(e)

        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ RenderSpec ë°˜í™˜ (ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œìš©)
        error_render_spec = {
            "type": "text",
            "title": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "text": {
                "content": f"## ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤\n\n"
                          f"**ìš”ì²­**: {request.message}\n\n"
                          f"**ì˜¤ë¥˜**: {str(e)}\n\n"
                          f"ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "format": "markdown",
                "sections": [
                    {
                        "type": "error",
                        "title": "ì˜¤ë¥˜ ì •ë³´",
                        "content": str(e)
                    }
                ]
            },
            "metadata": {
                "requestId": request_id,
                "generatedAt": datetime.utcnow().isoformat() + "Z"
            }
        }

        # ì—ëŸ¬ ì‹œ ë¹ˆ QueryResult ë°˜í™˜
        error_query_result = {
            "requestId": request_id,
            "status": "error",
            "data": {"rows": [], "aggregations": {}},
            "metadata": {
                "executionTimeMs": total_time,
                "rowsReturned": 0,
                "totalRows": 0,
                "dataSource": "error"
            },
            "error": {
                "code": "PROCESSING_ERROR",
                "message": str(e)
            }
        }

        # ì—ëŸ¬ ì‹œ ë¹ˆ QueryPlan ë°˜í™˜
        error_query_plan = {
            "entity": "",
            "operation": "list"
        }

        return ChatResponse(
            request_id=request_id,
            render_spec=error_render_spec,
            query_result=error_query_result,
            query_plan=error_query_plan,
            ai_message=f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            timestamp=datetime.utcnow().isoformat() + "Z"
        )


async def call_core_api(query_plan: Dict[str, Any]) -> Dict[str, Any]:
    """Core API í˜¸ì¶œ"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{CORE_API_URL}/api/v1/query/start",
                json=query_plan
            )

            # HTTP ì—ëŸ¬ê°€ ì•„ë‹Œ ë¹„ì¦ˆë‹ˆìŠ¤ ì—ëŸ¬ë„ ì²˜ë¦¬
            if response.status_code >= 400:
                error_body = response.json() if response.headers.get("content-type", "").startswith("application/json") else {}
                logger.warning(f"Core API returned {response.status_code}: {error_body}")
                return error_body if error_body else {
                    "status": "error",
                    "error": {
                        "code": f"HTTP_{response.status_code}",
                        "message": response.text
                    }
                }

            return response.json()

    except httpx.TimeoutException:
        logger.error("Core API timeout")
        return {
            "status": "error",
            "error": {
                "code": "TIMEOUT",
                "message": "Core API ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
            }
        }
    except httpx.HTTPError as e:
        logger.error(f"Core API HTTP error: {e}")
        return {
            "status": "error",
            "error": {
                "code": "CONNECTION_ERROR",
                "message": f"Core API ì—°ê²° ì˜¤ë¥˜: {str(e)}"
            }
        }


@router.get("/chat/test")
async def test_core_api():
    """Core API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{CORE_API_URL}/api/v1/query/health")
            response.raise_for_status()
            return {
                "core_api_status": "reachable",
                "core_api_response": response.json()
            }
    except httpx.HTTPError as e:
        return {
            "core_api_status": "unreachable",
            "error": str(e)
        }


@router.get("/chat/config")
async def get_config():
    """í˜„ìž¬ ì„¤ì • í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    return {
        "core_api_url": CORE_API_URL,
        "openai_api_key_set": bool(os.getenv("OPENAI_API_KEY")),
        "anthropic_api_key_set": bool(os.getenv("ANTHROPIC_API_KEY")),
        "rag_enabled": os.getenv("RAG_ENABLED", "true").lower() == "true",
        "database_url_set": bool(os.getenv("DATABASE_URL")),
        "query_plan_validation": {
            "enabled": ENABLE_QUERY_PLAN_VALIDATION,
            "validator_provider": os.getenv("VALIDATOR_LLM_PROVIDER", "openai"),
            "validator_model": os.getenv("VALIDATOR_LLM_MODEL", "gpt-4o-mini"),
            "quality_threshold": float(os.getenv("VALIDATOR_QUALITY_THRESHOLD", "0.8")),
            "use_llm_validation": os.getenv("VALIDATOR_USE_LLM", "true").lower() == "true"
        },
        "step": "8-query-plan-validation"
    }


@router.get("/chat/rag/status")
async def get_rag_status():
    """RAG ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        rag_service = get_rag_service()
        doc_counts = await rag_service.get_document_count()

        return {
            "status": "available",
            "rag_enabled": os.getenv("RAG_ENABLED", "true").lower() == "true",
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "document_counts": doc_counts,
            "total_documents": sum(doc_counts.values()) if doc_counts else 0
        }
    except Exception as e:
        return {
            "status": "unavailable",
            "error": str(e)
        }


@router.post("/chat/rag/search")
async def search_documents(query: str, k: int = 3):
    """RAG ë¬¸ì„œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        rag_service = get_rag_service()
        documents = await rag_service.search_docs(query=query, k=k)

        return {
            "query": query,
            "count": len(documents),
            "documents": [
                {
                    "id": doc.id,
                    "doc_type": doc.doc_type,
                    "title": doc.title,
                    "content": doc.content[:200] + "..." if len(doc.content) > 200 else doc.content,
                    "similarity": doc.similarity
                }
                for doc in documents
            ]
        }
    except Exception as e:
        return {
            "query": query,
            "error": str(e)
        }
